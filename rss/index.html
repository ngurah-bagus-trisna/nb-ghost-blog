<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[twnb]]></title><description><![CDATA[Tech with Ngurah Bagus]]></description><link>https://twnb.nbtrisna.my.id/</link><image><url>https://twnb.nbtrisna.my.id/favicon.png</url><title>twnb</title><link>https://twnb.nbtrisna.my.id/</link></image><generator>Ghost 5.125</generator><lastBuildDate>Tue, 24 Jun 2025 15:46:34 GMT</lastBuildDate><atom:link href="https://twnb.nbtrisna.my.id/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[24-June-25]]></title><description><![CDATA[Continue learn about AWS Lambda & Provisioning Lambda Function On AWS Using Terraform ]]></description><link>https://twnb.nbtrisna.my.id/24-june-25/</link><guid isPermaLink="false">685ac733dace4b00019e527b</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Tue, 24 Jun 2025 15:43:34 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/ba96ce4c-f7b8-429c-9b7f-cd9ea6ac6482.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="daily-quest-7-rds-serverless-foundationspart-2">Daily Quest #7: RDS &amp; Serverless Foundations - Part 2</h3><blockquote>AWS have service called <code>Lambda</code> for serverless application. Lambda can run code without provisioning or managing servers. Lambda runs your code on a high-availibilty compute infrastructure and manage all the computing resources, including servers and operating systems maintenance, capacity provisioning, automatic scaling &amp; loging.</blockquote><img src="https://photoby.nbtrisna.my.id/ba96ce4c-f7b8-429c-9b7f-cd9ea6ac6482.jpg" alt="24-June-25"><p>Reference :</p><ul><li>https://docs.aws.amazon.com/lambda/latest/dg/welcome.html</li></ul><p>All source code stored : <a href="https://github.com/ngurah-bagus-trisna/aws-vpc-iaac?ref=dev.nbtrisna.my.id">https://github.com/ngurah-bagus-trisna/aws-vpc-iaac</a></p><p>When using lambda, you are responsible only for code. Lambda mange the compute and other resources to run your code. In this journey, i want to create lambda for reporting access to database RDS and run simple <code>SELECT NOW();</code>, then put to S3 bucket by serializing result to JSON.</p><p>This is a squel to yersterday&apos;s article, about <a href="https://twnb.nbtrisna.my.id/22-june-25/?ref=dev.nbtrisna.my.id">RDS &amp; Serverless Foundations</a>. You may want to visit that article before continue</p><ol><li>Create bucket resource in <code>s3.tf</code></li></ol><p>This terraform resources, provisoning bucket called <code>nb-quest-reports</code> and attach public_access_block to deny all acl, policy &amp; securing access to bucket</p><pre><code class="language-hcl">resource &quot;aws_s3_bucket&quot; &quot;nb-quest-reports&quot; {
  bucket = &quot;nb-quest-reports&quot;
  tags = {
    &quot;Name&quot; = &quot;nb-quest-report&quot;
  }
}

resource &quot;aws_s3_bucket_public_access_block&quot; &quot;nb-quest-deny-access&quot; {
  bucket = aws_s3_bucket.nb-quest-reports.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

</code></pre><ol start="2"><li>Define IAM policy for lambda, allowing to access <code>ssm:GetParameter</code>, and <code>s3:PutObject</code>. Write into <code>lambda.tf</code></li></ol><p>First create an <code>aws_iam_poicy</code> called <code>lambdaPolicy</code>, that allows following actions :</p><ul><li>Put object to AWS S3 storage</li><li>Allow get parameter from AWS System Manager</li><li>Allow get secret from AWS Secrets Manager.</li></ul><p>After policy created, create role with effect <code>Allows</code> only Lambda service can use this role. Then attach new policy to role.</p><pre><code class="language-hcl">resource &quot;aws_iam_policy&quot; &quot;lambda-policy&quot; {
  name        = &quot;lambdaPolicy&quot;
  path        = &quot;/&quot;
  description = &quot;Allow write to S3 and read System Parameter&quot;

  policy = jsonencode({
    &quot;Version&quot; = &quot;2012-10-17&quot;,
    &quot;Statement&quot; = [
      {
        &quot;Sid&quot;    = &quot;AllowPutObject&quot;,
        &quot;Effect&quot; = &quot;Allow&quot;,
        &quot;Action&quot; = [
          &quot;s3:PutObject&quot;
        ],
        &quot;Resource&quot; = [
          &quot;*&quot;
        ]
      },
      {
        &quot;Sid&quot;    = &quot;AllowGetParameterSSM&quot;,
        &quot;Effect&quot; = &quot;Allow&quot;,
        &quot;Action&quot; = [
          &quot;ssm:GetParameter&quot;
        ],
        &quot;Resource&quot; = [
          &quot;*&quot;
        ]
      },
      {
        &quot;Sid&quot;    = &quot;AllowGetSecretValue&quot;,
        &quot;Effect&quot; = &quot;Allow&quot;,
        &quot;Action&quot; = [
          &quot;secretsmanager:GetSecretValue&quot;
        ],
        &quot;Resource&quot; = [
          &quot;*&quot;
        ]
      },
      {
        &quot;Sid&quot;: &quot;AllowManageENI&quot;,
        &quot;Effect&quot;: &quot;Allow&quot;,
        &quot;Action&quot;: [
          &quot;ec2:CreateNetworkInterface&quot;,
          &quot;ec2:DescribeNetworkInterfaces&quot;,
          &quot;ec2:DeleteNetworkInterface&quot;
        ],
        &quot;Resource&quot;: &quot;*&quot;
      },
      {
        &quot;Sid&quot;: &quot;AllowCloudWatchLogs&quot;,
        &quot;Effect&quot;: &quot;Allow&quot;,
        &quot;Action&quot;: [
          &quot;logs:CreateLogGroup&quot;,
          &quot;logs:CreateLogStream&quot;,
          &quot;logs:PutLogEvents&quot;
        ],
        &quot;Resource&quot;: &quot;*&quot;
      }
    ]
    }
  )
}

resource &quot;aws_iam_role&quot; &quot;reporting_lambda_role&quot; {
  depends_on = [aws_iam_policy.lambda-policy]
  name       = &quot;ReportingLambdaRole&quot;
  assume_role_policy = jsonencode({
    Version = &quot;2012-10-17&quot;,
    Statement = [
      {
        Effect = &quot;Allow&quot;
        Principal = {
          Service = &quot;lambda.amazonaws.com&quot;
        },
        Action = &quot;sts:AssumeRole&quot;
      }
    ]
  })
}

resource &quot;aws_iam_role_policy_attachment&quot; &quot;lambda_policy_attachment&quot; {
  depends_on = [
    aws_iam_policy.lambda-policy,
    aws_iam_role.reporting_lambda_role
  ]
  role       = aws_iam_role.reporting_lambda_role.name
  policy_arn = aws_iam_policy.lambda-policy.arn
}

resource &quot;aws_lambda_function&quot; &quot;db_to_s3_lambda&quot; {
  depends_on = [
    aws_db_instance.nb-db,
    aws_s3_bucket.nb-quest-reports,
    aws_iam_role_policy_attachment.lambda_policy_attachment
  ]
  function_name    = &quot;dbToS3Lambda&quot;
  handler          = &quot;app.lambda_handler&quot;
  runtime          = &quot;python3.12&quot;
  filename         = &quot;${path.module}/lambda.zip&quot;
  role             = aws_iam_role.reporting_lambda_role.arn
  source_code_hash = filebase64sha256(&quot;${path.module}/lambda.zip&quot;)
  timeout          = 10

  vpc_config {
    subnet_ids = [aws_subnet.nb-subnet[&quot;private-net-1&quot;].id]
    security_group_ids = [aws_security_group.rds-sg.id, aws_security_group.web-sg.id]
  }
  
  environment {
    variables = {
      SECRET_NAME = aws_db_instance.nb-db.master_user_secret[0].secret_arn
      BUCKET_NAME = aws_s3_bucket.nb-quest-reports.id
      DB_HOST     = aws_db_instance.nb-db.address
      DB_USER     = aws_db_instance.nb-db.username
      DB_NAME     = aws_db_instance.nb-db.db_name
    }
  }
}
</code></pre><ol start="3"><li>Create Lambda function.</li></ol><p>In this section i use python to create function with the following action :</p><ul><li>Read DB credentials (from Env / SSM / Secrets Manager)</li><li>Connect to the RDS endpoint</li><li>Run a simple <code>SELECT NOW();</code> query</li><li>Serialize the result to JSON and <code>PUT</code> it into your S3 bucket</li></ul><pre><code class="language-py">import boto3
import pymysql
import os
import json
import time
from datetime import datetime
from botocore.exceptions import ClientError

def lambda_handler(event, context):
    print(&quot;&#x1F680; Lambda started.&quot;)

    # Step 1: Load env vars
    try:
        secret_name = os.environ[&apos;SECRET_NAME&apos;]
        bucket_name = os.environ[&apos;BUCKET_NAME&apos;]
        db_host = os.environ[&apos;DB_HOST&apos;]
        db_user = os.environ[&apos;DB_USER&apos;]
        db_name = os.environ[&apos;DB_NAME&apos;]
        print(f&quot;&#x2705; Env vars loaded:\n  DB_HOST={db_host}\n  DB_USER={db_user}\n  DB_NAME={db_name}\n  BUCKET={bucket_name}&quot;)
    except KeyError as e:
        print(f&quot;&#x274C; Missing environment variable: {str(e)}&quot;)
        return {&quot;statusCode&quot;: 500, &quot;body&quot;: &quot;Missing environment variable&quot;}

    # Step 2: Get DB password from Secrets Manager
    try:
        secrets_client = boto3.client(&apos;secretsmanager&apos;)
        print(&quot;&#x1F50D; Fetching password from Secrets Manager...&quot;)
        start_time = time.time()
        secret_value = secrets_client.get_secret_value(SecretId=secret_name)
        password = json.loads(secret_value[&apos;SecretString&apos;])[&apos;password&apos;]
        print(f&quot;&#x2705; Password fetched in {round(time.time() - start_time, 2)}s&quot;)
    except ClientError as e:
        print(f&quot;&#x274C; Failed to get secret: {e.response[&apos;Error&apos;][&apos;Message&apos;]}&quot;)
        return {&quot;statusCode&quot;: 500, &quot;body&quot;: &quot;Failed to get DB password&quot;}
    except Exception as e:
        print(f&quot;&#x274C; Unexpected error getting secret: {str(e)}&quot;)
        return {&quot;statusCode&quot;: 500, &quot;body&quot;: &quot;Error while retrieving secret&quot;}

    # Step 3: Connect to DB
    try:
        print(&quot;&#x1F50C; Connecting to DB...&quot;)
        start_time = time.time()
        conn = pymysql.connect(
            host=db_host,
            user=db_user,
            password=password,
            db=db_name,
            connect_timeout=5
        )
        print(f&quot;&#x2705; Connected to DB in {round(time.time() - start_time, 2)}s&quot;)
    except Exception as e:
        print(f&quot;&#x274C; DB connection failed: {str(e)}&quot;)
        return {&quot;statusCode&quot;: 500, &quot;body&quot;: &quot;Failed to connect to database&quot;}

    # Step 4: Run SELECT NOW()
    try:
        print(&quot;&#x1F4E1; Executing query...&quot;)
        start_time = time.time()
        with conn.cursor() as cursor:
            cursor.execute(&quot;SELECT NOW();&quot;)
            result = cursor.fetchone()
        conn.close()
        print(f&quot;&#x2705; Query result: {result[0]} in {round(time.time() - start_time, 2)}s&quot;)
    except Exception as e:
        print(f&quot;&#x274C; Query failed: {str(e)}&quot;)
        return {&quot;statusCode&quot;: 500, &quot;body&quot;: &quot;DB query failed&quot;}

    # Step 5: Upload to S3
    try:
        print(&quot;&#x2601;&#xFE0F; Uploading to S3...&quot;)
        start_time = time.time()
        s3_client = boto3.client(&apos;s3&apos;)
        payload = json.dumps({&quot;timestamp&quot;: str(result[0])})
        filename = f&quot;timestamp_{datetime.utcnow().isoformat()}.json&quot;

        s3_client.put_object(
            Bucket=bucket_name,
            Key=filename,
            Body=payload
        )
        print(f&quot;&#x2705; Uploaded to S3: {filename} in {round(time.time() - start_time, 2)}s&quot;)
    except Exception as e:
        print(f&quot;&#x274C; Failed to upload to S3: {str(e)}&quot;)
        return {&quot;statusCode&quot;: 500, &quot;body&quot;: &quot;S3 upload failed&quot;}

    return {
        &quot;statusCode&quot;: 200,
        &quot;body&quot;: f&quot;Saved to S3 as {filename}&quot;
    }
</code></pre><ol start="4"><li>Plan + Provisioning to AWS</li></ol><pre><code class="language-sh">tofu plan 
tofu apply
</code></pre><h3 id="result">Result</h3><p>Test in AWS Lambda dashboard</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250624151837.png" class="kg-image" alt="24-June-25" loading="lazy" width="1490" height="555"></figure><p>Done fetch data from DB to S3 Storage</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250624145529.png" class="kg-image" alt="24-June-25" loading="lazy" width="1505" height="435"></figure><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250624145649.png" class="kg-image" alt="24-June-25" loading="lazy" width="494" height="113"></figure>]]></content:encoded></item><item><title><![CDATA[23-June-25]]></title><description><![CDATA[Learn Provisioning Infrastructure Using Terraform on AWS for EC2 and RDS.]]></description><link>https://twnb.nbtrisna.my.id/23-june-25/</link><guid isPermaLink="false">685986ebdace4b00019e526c</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Mon, 23 Jun 2025 16:56:41 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/f360beaa-fa84-4e16-8898-e5485cb673bd.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="daily-quest-7-rds-serverless-foundations">Daily Quest #7: RDS &amp; Serverless Foundations</h2><blockquote>Amazon RDS (Relational Database Service) is a web service that makes it easier to setup, operate, and scale a relational database in the AWS Cloud. AWS have feature to run code without provisioning and managing server. It&apos;s callled Lambda for serverless services. So today i learn to provisioning infrastructure App Server on EC2, MySQL RDS, and reporting using lambda write result an S3 Bucket</blockquote><img src="https://photoby.nbtrisna.my.id/f360beaa-fa84-4e16-8898-e5485cb673bd.jpg" alt="23-June-25"><p>Reference :</p><ul><li>https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html</li><li>https://docs.aws.amazon.com/lambda/latest/dg/welcome.html</li></ul><p><strong>Skenario :</strong> Build VPC and IAM mastery by standing up a private RDS database and a serverless reporting Lambda&#x2014;glued together with least-privilege IAM.</p><p>All source code stored in :</p><ul><li>https://github.com/ngurah-bagus-trisna/aws-vpc-iaac</li></ul><ol><li>Create <code>provider.tf</code></li></ol><p>This file store about provider/extention for resources needed. In this i&apos;m using provider from <code>hashicorp/aws</code></p><pre><code class="language-tf file:provider.tf">terraform {
  required_providers {
    aws = {
      source  = &quot;hashicorp/aws&quot;
      version = &quot;~&gt; 5.0&quot;
    }
  }
}

# Configure the AWS Provider
provider &quot;aws&quot; {
  region = &quot;ap-southeast-1&quot;
}
</code></pre><ol start="2"><li>Create <code>variable.tf</code> for mapping variable and data type for value stored on variable.</li></ol><pre><code class="language-tf file:variable.tf">variable &quot;vpc_cidr&quot; {
  type = string
}

variable &quot;subnet&quot; {
  type = map(object({
    subnet_range      = string
    availability_zone = string
    type              = string
  }))
}

variable &quot;natgw_name&quot; {
  type = string
}

variable &quot;route_tables&quot; {
  type = map(
    object({
      cidr_source       = string
      route_destination = string
    })
  )
}

variable &quot;db_credentials&quot; {
  type = object({
    username = string
    password = string
  })
  sensitive = true
}

variable &quot;instance&quot; {
  type = map(object({
    ami           = string
    instance_type = string
    subnet        = string
  }))
}
</code></pre><ol start="3"><li>Create <code>vpc.tf</code> file. This file contain VPC (Virtual Private Cloud) infrasturcture resource want to create. Resources i want to create :</li></ol><ul><li>VPC with subnet <code>10.0.0.0/16</code></li><li>1 Public subnet, with range ip <code>10.0.1.0/24</code></li><li>3 Private subnet, with range ip <code>10.0.2.0/24, 10.0.3.0/24, 10.0.4.0/24</code> with different availibilty zone</li><li>1 AWS Nat Gateway for Private Subnet</li><li>1 AWS ElasticIP for eggress Nat Gateway</li><li>1 AWS Internet Gateway, for public subnet</li><li>2 Routing, each for public subnet and private subnet</li><li>2 Security group, one for ingress port <code>22</code>, and second to ingress <code>rds</code>/database subnet</li></ul><pre><code class="language-tf file:vpc.tf">resource &quot;aws_vpc&quot; &quot;nb-chatgpt-vpc&quot; {
  cidr_block = var.vpc_cidr
  tags = {
    &quot;Name&quot; = &quot;nb-chatgpt-vpc&quot;
  }
}

resource &quot;aws_subnet&quot; &quot;nb-subnet&quot; {
  depends_on = [aws_vpc.nb-chatgpt-vpc]
  vpc_id     = aws_vpc.nb-chatgpt-vpc.id
  for_each   = var.subnet

  cidr_block              = each.value.subnet_range
  availability_zone       = each.value.availability_zone
  map_public_ip_on_launch = each.key == &quot;public-net&quot; ? true : false
  tags = {
    &quot;Name&quot; = each.key
    &quot;Type&quot; = each.value.type
  }
}

resource &quot;aws_internet_gateway&quot; &quot;nb-inet-gw&quot; {
  depends_on = [aws_vpc.nb-chatgpt-vpc]
  vpc_id     = aws_vpc.nb-chatgpt-vpc.id

  tags = {
    Name = &quot;nb-inet-gw&quot;
  }
}

resource &quot;aws_eip&quot; &quot;nb-eip-nat-gw&quot; {
  depends_on = [aws_internet_gateway.nb-inet-gw]
  tags = {
    &quot;Name&quot; = &quot;nb-eip-nat-gw&quot;
  }
}


resource &quot;aws_nat_gateway&quot; &quot;nb-nat-gw&quot; {
  depends_on        = [aws_eip.nb-eip-nat-gw]
  allocation_id     = aws_eip.nb-eip-nat-gw.id
  subnet_id         = aws_subnet.nb-subnet[&quot;public-net&quot;].id
  connectivity_type = &quot;public&quot;
  tags = {
    &quot;Name&quot; : var.natgw_name
  }
}


resource &quot;aws_route_table&quot; &quot;public&quot; {
  vpc_id = aws_vpc.nb-chatgpt-vpc.id

  route {
    cidr_block = &quot;0.0.0.0/0&quot;
    gateway_id = aws_internet_gateway.nb-inet-gw.id
  }

  tags = {
    Name = &quot;public-rt&quot;
  }
}

resource &quot;aws_route_table_association&quot; &quot;public&quot; {
  subnet_id      = aws_subnet.nb-subnet[&quot;public-net&quot;].id
  route_table_id = aws_route_table.public.id
}


resource &quot;aws_route_table&quot; &quot;private&quot; {
  vpc_id = aws_vpc.nb-chatgpt-vpc.id

  route {
    cidr_block     = &quot;0.0.0.0/0&quot;
    nat_gateway_id = aws_nat_gateway.nb-nat-gw.id
  }

  tags = {
    Name = &quot;private-rt&quot;
  }
}

resource &quot;aws_route_table_association&quot; &quot;private&quot; {
  for_each = {
    for key, subnet in var.subnet :
    key =&gt; subnet
    if subnet.type == &quot;private&quot;
  }

  subnet_id      = aws_subnet.nb-subnet[each.key].id
  route_table_id = aws_route_table.private.id
}

resource &quot;aws_security_group&quot; &quot;web-sg&quot; {
  depends_on = [aws_subnet.nb-subnet]

  name        = &quot;web-sg&quot;
  description = &quot;Security group to allow access port 22&quot;
  vpc_id      = aws_vpc.nb-chatgpt-vpc.id

  tags = {
    &quot;Name&quot; : &quot;web-server-sg&quot;
  }
}

resource &quot;aws_vpc_security_group_ingress_rule&quot; &quot;allow-access-ssh&quot; {
  depends_on = [aws_security_group.web-sg]

  security_group_id = aws_security_group.web-sg.id
  cidr_ipv4         = &quot;0.0.0.0/0&quot;
  to_port           = 22
  from_port         = 22
  ip_protocol       = &quot;tcp&quot;
}
</code></pre><ol start="4"><li>Create <code>rds.tf</code> for RDS / Database infrastructure.</li></ol><p>First i create resource <code>aws_db_subnet_group</code> for mapping subnet allowed access from rds, and tell what subnet using for instance_db. Second i create security group for accessing rds from private subnet. Last i created resource <code>aws_db_instance</code> for provisioning one RDS</p><pre><code class="language-tf file:rds.tf">resource &quot;aws_db_subnet_group&quot; &quot;nb-db-subnet&quot; {
  depends_on = [aws_subnet.nb-subnet]
  name       = &quot;nb-db-subnet&quot;
  subnet_ids = [
    for key, subnet in var.subnet : aws_subnet.nb-subnet[key].id
    if subnet.type == &quot;private&quot;
  ]

  tags = {
    &quot;Name&quot; = &quot;Private DB Subnet Group&quot;
  }
}


resource &quot;aws_security_group&quot; &quot;rds-sg&quot; {
  depends_on  = [aws_subnet.nb-subnet]
  name        = &quot;rds-sg&quot;
  description = &quot;Security group to allow access rds-subnet from private subnets&quot;
  vpc_id      = aws_vpc.nb-chatgpt-vpc.id

  tags = {
    Name = &quot;rds-server-sg&quot;
  }
}

resource &quot;aws_vpc_security_group_ingress_rule&quot; &quot;allow-access-rds&quot; {
  depends_on        = [aws_security_group.rds-sg]
  security_group_id = aws_security_group.rds-sg.id
  cidr_ipv4         = aws_subnet.nb-subnet[&quot;private-net-1&quot;].cidr_block
  from_port         = 3306
  to_port           = 3306
  ip_protocol       = &quot;tcp&quot;
}

resource &quot;aws_db_instance&quot; &quot;nb-db&quot; {
  depends_on             = [aws_security_group.rds-sg, aws_vpc_security_group_ingress_rule.allow-access-rds]
  allocated_storage      = 10
  db_name                = &quot;nbdb&quot;
  engine                 = &quot;mysql&quot;
  instance_class         = &quot;db.t3.micro&quot;
  username               = var.db_credentials.username
  password               = var.db_credentials.password
  publicly_accessible    = false
  vpc_security_group_ids = [aws_security_group.rds-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.nb-db-subnet.name
  skip_final_snapshot    = true
}
</code></pre><ol start="5"><li>Create EC2 Instance for public-web</li></ol><p>First create <code>aws_network_interface</code> for instance created. Then provisioning instance with <code>aws_instance</code> resources.</p><pre><code class="language-tf file:ec2.tf">
resource &quot;aws_network_interface&quot; &quot;instance-interface&quot; {
  depends_on      = [aws_subnet.nb-subnet]
  for_each        = var.instance
  subnet_id       = aws_subnet.nb-subnet[each.value.subnet].id
  security_groups = [aws_security_group.web-sg.id]

  tags = {
    &quot;Name&quot; = &quot;interface ${each.key}&quot;
  }
}

resource &quot;aws_instance&quot; &quot;nb-instance&quot; {
  for_each   = var.instance
  depends_on = [aws_network_interface.instance-interface]

  ami           = each.value.ami
  instance_type = each.value.instance_type
  key_name      = &quot;nb-key&quot;

  network_interface {
    network_interface_id = aws_network_interface.instance-interface[each.key].id
    device_index         = 0
  }

  tags = {
    &quot;Name&quot; = &quot;Instance - ${each.key}&quot;
  }
}
</code></pre><ol start="6"><li>Create value file called <code>dev.tfvars</code> for storing all value needed in terraform file</li></ol><pre><code class="language-tf file:dev.tfvars">vpc_cidr = &quot;10.0.0.0/16&quot;
subnet = {
  &quot;public-net&quot; = {
    subnet_range      = &quot;10.0.1.0/24&quot;
    availability_zone = &quot;ap-southeast-1a&quot;
    type              = &quot;public&quot;
  },
  &quot;private-net-1&quot; = {
    subnet_range      = &quot;10.0.2.0/24&quot;
    availability_zone = &quot;ap-southeast-1c&quot;
    type              = &quot;private&quot;
  },
  &quot;private-net-2&quot; = {
    subnet_range      = &quot;10.0.3.0/24&quot;
    availability_zone = &quot;ap-southeast-1b&quot;
    type              = &quot;private&quot;
  },
  &quot;private-net-3&quot; = {
    subnet_range      = &quot;10.0.4.0/24&quot;
    availability_zone = &quot;ap-southeast-1a&quot;
    type              = &quot;private&quot;
  }
}

natgw_name = &quot;nb-natgw&quot;

route_tables = {
  &quot;private&quot; = {
    cidr_source       = &quot;0.0.0.0/0&quot;
    route_destination = &quot;nat&quot;
  },
  &quot;public&quot; = {
    cidr_source       = &quot;0.0.0.0/0&quot;
    route_destination = &quot;igw&quot;
  }
}

instance = {
  &quot;web-public&quot; = {
    ami           = &quot;ami-02c7683e4ca3ebf58&quot;
    instance_type = &quot;t2.micro&quot;
    subnet        = &quot;public-net&quot;
  }
}
</code></pre><h2 id="result">Result</h2><p>For today, i only test to hit RDS from private instance.</p><pre><code class="language-sh"># installing mysql-client
sudo apt install mysql-client

# testing to login rds
mysql -h &lt;endpoint-rds&gt; -u &lt;user&gt; -p
</code></pre><p>Result, can connect to rds from private subnet.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250624090557.png" class="kg-image" alt="23-June-25" loading="lazy" width="1212" height="251"></figure>]]></content:encoded></item><item><title><![CDATA[22-June-25]]></title><description><![CDATA[Learn IAM Roles, EC2 Instance Profile & Provisioning VPC using Iaac (Terraform)]]></description><link>https://twnb.nbtrisna.my.id/22-june-25/</link><guid isPermaLink="false">6857ffdbdace4b00019e525c</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Sun, 22 Jun 2025 13:08:10 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/1e929f06-0136-4b70-8924-cd9e66ff281b.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="aws-iam-roles-ec2-instance-profiles">AWS IAM Roles &amp; EC2 Instance Profiles</h2><blockquote>Application that run on EC2 Instance must include AWS credentials in AWS API request. You could have developers to uplaod credentials directly to instances, but developer need to check again the credential can securely access AWS API and update each amaszon credential when time will come. It&apos;s painfull workflow.</blockquote><img src="https://photoby.nbtrisna.my.id/1e929f06-0136-4b70-8924-cd9e66ff281b.jpg" alt="22-June-25"><p>Instead you can add should use IAM role to manage temporary credentials for application that run on EC2 instance. When use a role, you don&apos;t have to distribute long-term credentials (like sign-in credentials or access keys).</p><p>Reference :</p><ul><li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html?ref=dev.nbtrisna.my.id">Use IAM role to grant permissiont to application running on EC2 Instances</a></li><li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html?ref=dev.nbtrisna.my.id">Instance Metadata Services</a></li></ul><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622091858.png" class="kg-image" alt="22-June-25" loading="lazy" width="621" height="296"></figure><blockquote><strong>Real world usecase :</strong> A web server running on EC2 needs to fetch secrets on AWS System Manager Parameter Store (SSM) and upload logs to S3 bucket-without embedding long-lived api keys in your AMI</blockquote><p><strong>Skenario</strong> : Design IAM Role <code>WebServerRole</code> that allows only <code>ssm:GetParameter</code>, <code>s3:PutObject</code>.</p><h3 id="create-role-webserverrole-with-policy-allows-ssmgetparameter-s3putobject-to-one-bucket-prefix">Create role <code>WebServerRole</code>, with policy allows <code>ssm:GetParameter</code>, <code>s3:PutObject</code> to one bucket prefix</h3><h4 id="s3-bukcet">S3 Bukcet</h4><blockquote>Amazon S3 is an object storage service that offers industry-leading scalability, data availability, security, and performance</blockquote><p>To create bucket first navigate to <code>S3</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622095240.png" class="kg-image" alt="22-June-25" loading="lazy" width="2040" height="1030"></figure><p>Click <code>Create S3</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622095313.png" class="kg-image" alt="22-June-25" loading="lazy" width="2676" height="632"></figure><p>Begin to setup new bucket</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622095618.png" class="kg-image" alt="22-June-25" loading="lazy" width="3254" height="1834"></figure><blockquote>Bucket name must uniqe becaus after a bucket is created, the name of that bucket cannot be used by another AWS account in any AWS Region until the bucket is deleted</blockquote><p>Leave everything with default configuration, and click <code>Create Bucket</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622095729.png" class="kg-image" alt="22-June-25" loading="lazy" width="3254" height="1834"></figure><p>Bucket created.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622101907.png" class="kg-image" alt="22-June-25" loading="lazy" width="3266" height="972"></figure><h4 id="role">Role</h4><p>Create an IAM Role <code>WebServerRole</code> that&apos;s allows only <code>ssm:GetParameter</code>, <code>s3:PutObject</code>.</p><p>Create policy first, with name <code>GetParameterPutObject</code>. Navigate to <code>Identity and Access Management (IAM) &gt; Policies &gt; Create Policies</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622102313.png" class="kg-image" alt="22-June-25" loading="lazy" width="3262" height="794"></figure><p>We configure policy to allow GetParameter on spesific parameter store path on any region</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622102635.png" class="kg-image" alt="22-June-25" loading="lazy" width="3260" height="1776"></figure><p>Then we add allow <code>S3:PutObject</code> to spesific bucket we created</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622102829.png" class="kg-image" alt="22-June-25" loading="lazy" width="2776" height="1620"></figure><p>Review all, when seems correct, begin to create policy</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622103010.png" class="kg-image" alt="22-June-25" loading="lazy" width="3268" height="1772"></figure><p>Policy created</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622103047.png" class="kg-image" alt="22-June-25" loading="lazy" width="3270" height="872"></figure><p>Then we assign new policy to role <code>WebServerRole</code>. Navigate to <code>Identity and Access Management (IAM) &gt; Roles &gt; Create Role</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622102055.png" class="kg-image" alt="22-June-25" loading="lazy" width="3268" height="1204"></figure><p>Configure trusted entity type to Aws Service, and select EC2. <em>Because this role attached to EC2 Instances</em></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622103240.png" class="kg-image" alt="22-June-25" loading="lazy" width="3260" height="1260"></figure><p>We need to checklist policy we want to attached to role</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622103338.png" class="kg-image" alt="22-June-25" loading="lazy" width="2748" height="968"></figure><p>Review again, if all corrected, begin to create role</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622103432.png" class="kg-image" alt="22-June-25" loading="lazy" width="2762" height="1670"></figure><p>Role <code>WebServerRole</code> created.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622104414.png" class="kg-image" alt="22-June-25" loading="lazy" width="3262" height="820"></figure><h4 id="test-launch-instance-and-attach-webserverrole">Test, launch instance and attach <code>WebServerRole</code></h4><p>When provisioning instance, we need to attach <code>WebServerRole</code> in IAM Profile by access to <code>Advanced Details &gt; IAM Instance Profile</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622104706.png" class="kg-image" alt="22-June-25" loading="lazy" width="2036" height="712"></figure><p>Check role attached using IMDSv2</p><pre><code class="language-sh">TOKEN=`curl -X PUT &quot;http://169.254.169.254/latest/api/token&quot; -H &quot;X-aws-ec2-metadata-token-ttl-seconds: 21600&quot;`

curl -H &quot;X-aws-ec2-metadata-token: $TOKEN&quot; \
  http://169.254.169.254/latest/meta-data/iam/security-credentials/
</code></pre><p>Result, successfuly to attach role <code>WebServerRole</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622105547.png" class="kg-image" alt="22-June-25" loading="lazy" width="1420" height="160"></figure><p>Trying to get parameter, <code>nb-test</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622105949.png" class="kg-image" alt="22-June-25" loading="lazy" width="3106" height="640"></figure><pre><code class="language-sh">aws ssm get-parameter
</code></pre><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622110111.png" class="kg-image" alt="22-June-25" loading="lazy" width="1536" height="424"></figure><p>How about <code>delete-parameter</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622110155.png" class="kg-image" alt="22-June-25" loading="lazy" width="3276" height="182"></figure><p>Denied, because doesn&apos;t have policy <code>ssm:DeleteParameter</code></p><p>Try to put <code>helo</code> on S3 Bucket</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622110652.png" class="kg-image" alt="22-June-25" loading="lazy" width="1458" height="140"></figure><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622110703.png" class="kg-image" alt="22-June-25" loading="lazy" width="2734" height="772"></figure><h2 id="daily-quest-5-%E2%80%9Cterraforming-the-multi-tier-vpc%E2%80%9D">Daily Quest #5: &#x201C;Terraforming the Multi-Tier VPC&#x201D;</h2><blockquote>Terraform is infrastructure as a code tool. Allow user to create anything resources using a declarative code typically HashiCorp Configuration Language (HCL). This can make infrastructure more flexible using code instead of accessing console again-and again.</blockquote><p>Reference :</p><ul><li><a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs?ref=dev.nbtrisna.my.id">AWS Providers Terraform</a></li><li><a href="https://developer.hashicorp.com/terraform/tutorials/aws-get-started?ref=dev.nbtrisna.my.id">https://developer.hashicorp.com/terraform/tutorials/aws-get-started</a></li><li><a href="https://github.com/ngurah-bagus-trisna/aws-vpc-iaac?ref=dev.nbtrisna.my.id">https://github.com/ngurah-bagus-trisna/aws-vpc-iaac</a></li></ul><p>Infrastructure as Code (IaC) tools allow you to manage infrastructure with configuration files rather than through a graphical user interface. IaC allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share.</p><p>So in this section, i want to create IaaC base from <a href="https://twnb.nbtrisna.my.id/21-june-25/?ref=dev.nbtrisna.my.id">this article</a> section <em>Daily Quest #3: AWS Networking &amp; VPC Deep Dive</em></p><h2 id="setup-opentofu">Setup OpenTofu</h2><blockquote>Opentofu is opensource alternative for terraform.</blockquote><p>Reference :</p><ul><li>https://opentofu.org/docs/intro/install/</li></ul><p>I&apos;m using <em>Ubuntu</em> os so i prefer to chose <code>deb</code> install method</p><pre><code class="language-sh">curl --proto &apos;=https&apos; --tlsv1.2 -fsSL https://get.opentofu.org/install-opentofu.sh -o install-opentofu.sh

chmod +x install-opentofu.sh

./install-opentofu.sh --install-method deb

# verify install
tofu version
</code></pre><p>Result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622151825.png" class="kg-image" alt="22-June-25" loading="lazy" width="308" height="132"></figure><h3 id="setup-vpc">Setup VPC</h3><ol><li>Create file <code>main.tf</code> with AWS provider and region</li></ol><pre><code class="language-tf file:main.tf">terraform {
  required_providers {
    aws = {
      source  = &quot;hashicorp/aws&quot;
      version = &quot;~&gt; 5.0&quot;
    }
  }
}

# Configure the AWS Provider
provider &quot;aws&quot; {
  region = &quot;ap-southeast-1&quot;
}
</code></pre><ol start="2"><li>Setup variable.</li></ol><p>On terraform, input variable let you customize aspects of modules without altering the module&apos;s own source code. This function allows to share modules accrost different terraform configuration. Create file called <code>variable.tf</code>. <a href="https://opentofu.org/docs/language/values/variables/?ref=dev.nbtrisna.my.id#declaring-an-input-variable">Reference</a></p><pre><code class="language-tf file:variable.tf">variable &quot;vpc_cidr&quot; {
  type = string
}

variable &quot;subnet&quot; {
  type = map(object({
    subnet_range      = string
    availability_zone = string
    type              = string
  }))
}

variable &quot;natgw_name&quot; {
  type = string
}

variable &quot;route_tables&quot; {
  type = map(
    object({
      cidr_source       = string
      route_destination = string
    })
  )
}
</code></pre><p>Then setup <code>main.tf</code></p><pre><code class="language-tf file:main.tf">terraform {
  required_providers {
    aws = {
      source  = &quot;hashicorp/aws&quot;
      version = &quot;~&gt; 5.0&quot;
    }
  }
}

# Configure the AWS Provider
provider &quot;aws&quot; {
  region = &quot;ap-southeast-1&quot;
}

resource &quot;aws_vpc&quot; &quot;nb-chatgpt-vpc&quot; {
  cidr_block = var.vpc_cidr
  tags = {
    &quot;Name&quot; = &quot;nb-chatgpt-vpc&quot;
  }
}

resource &quot;aws_subnet&quot; &quot;nb-subnet&quot; {
  vpc_id   = aws_vpc.nb-chatgpt-vpc.id
  for_each = var.subnet

  cidr_block        = each.value.subnet_range
  availability_zone = each.value.availability_zone
  tags = {
    &quot;Name&quot; = each.key
    &quot;Type&quot; = each.value.type
  }
}

resource &quot;aws_internet_gateway&quot; &quot;nb-inet-gw&quot; {
  vpc_id = aws_vpc.nb-chatgpt-vpc.id

  tags = {
    Name = &quot;nb-inet-gw&quot;
  }
}

resource &quot;aws_eip&quot; &quot;nb-eip-nat-gw&quot; {
  tags = {
    &quot;Name&quot; = &quot;nb-eip-nat-gw&quot;
  }
}

locals {
  public_subnet_ids = [
    for key, subnet in var.subnet : aws_subnet.nb-subnet[key].id
    if subnet.type == &quot;public&quot;
  ]
  private_subnet_ids = [
    for key, subnet in var.subnet : aws_subnet.nb-subnet[key].id
    if subnet.type == &quot;private&quot;
  ]
}

resource &quot;aws_nat_gateway&quot; &quot;nb-nat-gw&quot; {
  depends_on        = [aws_eip.nb-eip-nat-gw]
  allocation_id     = aws_eip.nb-eip-nat-gw.id
  subnet_id         = local.public_subnet_ids[0]
  connectivity_type = &quot;public&quot;
  tags = {
    &quot;Name&quot; : var.natgw_name
  }
}

resource &quot;aws_route_table&quot; &quot;net-public&quot; {
  for_each = var.route_tables
  vpc_id   = aws_vpc.nb-chatgpt-vpc.id

  route {
    cidr_block     = each.value.cidr_source
    gateway_id     = each.value.route_destination == &quot;igw&quot; ? aws_internet_gateway.nb-inet-gw.id : null
    nat_gateway_id = each.value.route_destination == &quot;nat&quot; ? aws_nat_gateway.nb-nat-gw.id : null
  }
}

resource &quot;aws_route_table_association&quot; &quot;public&quot; {
  subnet_id      = aws_subnet.nb-subnet[&quot;public-net&quot;].id
  route_table_id = aws_route_table.net-public[&quot;public&quot;].id
}

resource &quot;aws_route_table_association&quot; &quot;private&quot; {
  subnet_id      = aws_subnet.nb-subnet[&quot;private-net&quot;].id
  route_table_id = aws_route_table.net-public[&quot;private&quot;].id
}

output &quot;vpc_id&quot; {
  value = aws_vpc.nb-chatgpt-vpc.id
}

output &quot;public_subnet_id&quot; {
  value = local.public_subnet_ids
}

output &quot;private_subnet_id&quot; {
  value = local.private_subnet_ids
}

output &quot;nat_gateway_public_ip&quot; {
  value = aws_nat_gateway.nb-nat-gw.public_ip
}
</code></pre><p>Then setup all value for variable in <code>dev.tfvars</code></p><pre><code class="language-tfvars file:dev.tfvars">vpc_cidr = &quot;10.0.0.0/16&quot;
subnet = {
  &quot;public-net&quot; = {
    subnet_range      = &quot;10.0.1.0/24&quot;
    availability_zone = &quot;ap-southeast-1c&quot;
    type              = &quot;public&quot;
  },
  &quot;private-net&quot; = {
    subnet_range      = &quot;10.0.2.0/24&quot;
    availability_zone = &quot;ap-southeast-1c&quot;
    type              = &quot;private&quot;
  }
}

natgw_name = &quot;nb-natgw&quot;

route_tables = {
  &quot;private&quot; = {
    cidr_source       = &quot;0.0.0.0/0&quot;
    route_destination = &quot;nat&quot;
  },
  &quot;public&quot; = {
    cidr_source       = &quot;0.0.0.0/0&quot;
    route_destination = &quot;igw&quot;
  }
}

</code></pre><p>Then Init, plan, apply</p><pre><code class="language-sh">tofu init
tofu plan -var-file=dev.tfvars

# if everything correct, apply to create resources
tofu apply
</code></pre><p>Result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622193558.png" class="kg-image" alt="22-June-25" loading="lazy" width="504" height="240"></figure><h3 id="testing">Testing</h3><p>Create 2 ec2 instances</p><p><strong>Instance : public-net</strong>, Attach public subnet, and attach elastic-ip for access. Access to instances, check public_ip,</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622194131.png" class="kg-image" alt="22-June-25" loading="lazy" width="2348" height="1482"></figure><p><em>public-net</em> instance using same as <em>public_ipv4</em> attached to instance.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622194500.png" class="kg-image" alt="22-June-25" loading="lazy" width="1468" height="468"></figure><p>Then access <em>private-net</em> instances from <em>public-net</em>. Makesure public ip using nat gateway address.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250622194835.png" class="kg-image" alt="22-June-25" loading="lazy" width="1236" height="476"></figure><p>Using nat-gateway address.</p>]]></content:encoded></item><item><title><![CDATA[21-June-25]]></title><description><![CDATA[Learn IAM Policies & AWS Networking]]></description><link>https://twnb.nbtrisna.my.id/21-june-25/</link><guid isPermaLink="false">6856af2fdace4b00019e524d</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Sat, 21 Jun 2025 13:11:11 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/820cd488-1486-482a-932f-08f6e63d1fc1.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="daily-quest-2-least-privilege-iam-policies">Daily Quest #2: Least-Privilege IAM Policies</h2><blockquote>IAM (Identity &amp; Access Management) is a web service that helps you securely control access to AWS Resources. On this day, i learn to craft and attach the minimal set of permissions erquired for EC-2 based application, following the principle of least privilage</blockquote><img src="https://photoby.nbtrisna.my.id/820cd488-1486-482a-932f-08f6e63d1fc1.jpg" alt="21-June-25"><p><strong>Reference</strong> :</p><ul><li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html?ref=dev.nbtrisna.my.id">What is IAM?</a></li><li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html?ref=dev.nbtrisna.my.id">Security best practices in IAM</a> &lt;- Recomended to read this for first time using AWS Resources</li></ul><p><strong>Skenario :</strong> Create <code>IAM Policy</code> to only permit <code>DescribeInstances, DescribeSubnets, DescribeVpcs</code>, And scope to aws account&apos;s resources.</p><h3 id="iam-policy">IAM Policy</h3><blockquote>IAM Policy is feature to manage access in AWS. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions</blockquote><p>Reference :</p><ul><li><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html?ref=dev.nbtrisna.my.id">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></li></ul><p>Access to IAM Policy</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621113842.png" class="kg-image" alt="21-June-25" loading="lazy" width="1062" height="527"></figure><p>Then open <code>Policy</code> menu in <code>IAM Dashboard</code>, select <code>Create Policy</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621114121.png" class="kg-image" alt="21-June-25" loading="lazy" width="1863" height="613"></figure><p>Select what resources to <code>Allow</code> or deny to access. On this section, i want to allow policy to <code>Describe instances, subnets, and VPCs</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621114539.png" class="kg-image" alt="21-June-25" loading="lazy" width="1855" height="930"></figure><p>After all correct, begin to create Policy be click <code>Create Policy</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621114707.png" class="kg-image" alt="21-June-25" loading="lazy" width="1868" height="908"></figure><h3 id="iam-role">IAM Role</h3><blockquote>Object on AWS to assign spesific permission to account.</blockquote><p>Now after policy created, we can attach policy to <code>IAM Role</code>. Navigate to <code>Roles &gt; Create Role</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621120646.png" class="kg-image" alt="21-June-25" loading="lazy" width="1613" height="700"></figure><p>Then configure trusted entity. For this, i configure entity type to <code>AWS Service</code>, and Usecase to <code>EC2</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621120818.png" class="kg-image" alt="21-June-25" loading="lazy" width="1851" height="915"></figure><p>Search permission policy, and next</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621120954.png" class="kg-image" alt="21-June-25" loading="lazy" width="1603" height="838"></figure><p>Enter name of roles, check again. If all correct, begin to create Role.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621121129.png" class="kg-image" alt="21-June-25" loading="lazy" width="1600" height="946"></figure><h3 id="check-if-role-correct">Check If Role Correct</h3><p>To test role, we need to attach <code>instance-profile</code> to VM. And then testing to <code>describe</code> and <code>terminate</code></p><p>Access to <code>EC2 &gt; Instances</code> to assign new role.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621124108.png" class="kg-image" alt="21-June-25" loading="lazy" width="1876" height="575"></figure><p>Attach Role &gt; and update IAM role</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621124135.png" class="kg-image" alt="21-June-25" loading="lazy" width="1528" height="341"></figure><blockquote>To test, you need to access instances/vm via ssh.</blockquote><h3 id="testing">Testing</h3><p>Testing to describe-instances</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621131604.png" class="kg-image" alt="21-June-25" loading="lazy" width="724" height="349"></figure><p>Testing destroy</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621131544.png" class="kg-image" alt="21-June-25" loading="lazy" width="1176" height="209"></figure><blockquote><strong>Real World Usecase</strong> : Using IAM Roles with the principle of least privilege on EC2 instances is essential for maintaining security and operational efficiency. For example, a web application hosted on an EC2 instance may need to access files stored in Amazon S3, such as images or configuration data. Instead of hardcoding access keys&#x2014;which poses security risks and is difficult to manage&#x2014;an IAM Role allows the EC2 instance to securely access these resources using temporary credentials managed automatically by AWS</blockquote><p><strong>Question :</strong></p><ul><li><strong>Why is it dangerous</strong> to attach <code>AdministratorAccess</code> to long-lived roles or CI/CD runners?</li><li><strong>How would you modify</strong> this policy if a Lambda function needed <strong>read-only</strong> access to <strong>only</strong> a specific S3 bucket?</li></ul><p><strong>Answer</strong> :</p><ul><li>Attach <code>AdministratorAccess</code> to ci/cd runners can potentialy have risk to manage all aws resources in instance without strict permission</li><li>I need to create policy called <code>S3ReadWrite</code> with policy allow to readwrite S3 storage, and associate to lamda (i never try before, but i get the point using least-privilage)</li></ul><h2 id="daily-quest-3-aws-networking-vpc-deep-dive">Daily Quest #3: AWS Networking &amp; VPC Deep Dive</h2><blockquote>Multi-tier network (public and private subnets) use for production use-case like connecting web servers to database. You don&apos;t need web-server comunicate to database exposed to internet. Instead we can use <code>private-network</code> to connect beetween web-server and database. To expose web-server we can use <code>internet-gateway</code>.</blockquote><p>Reference :</p><ul><li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/working-with-vpcs.html?ref=dev.nbtrisna.my.id#AddaSubnet">Create Subnet AWS</a></li><li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html?ref=dev.nbtrisna.my.id">Nat Gateways</a></li></ul><p><strong>Real World Usecase</strong> : In production web server and database recomend to use private network when comunicate each other. Using private network, need <code>Nat Gateway</code> to access intenet without exposing directly to the internet.</p><p><strong>Skenario</strong> : Creating 2 instances, first instance alocated Elastic IP and second instance doesn&apos;t attach elasticIP. Two instances connected between private-network. Makesure all instances can access internet</p><h3 id="subnet">Subnet</h3><blockquote>Subnet is range IP Addresses in VPC</blockquote><p>Create new subnet <code>10.0.2.0/24</code>, but mark it as <em>private</em> it means doesn&apos;t need to associate external gateway. Navigate to <code>VPC &gt; Subnets</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621175908.png" class="kg-image" alt="21-June-25" loading="lazy" width="1200" height="714"></figure><p>To create, click <code>Create Subnet</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621180215.png" class="kg-image" alt="21-June-25" loading="lazy" width="1612" height="421"></figure><p>Configure subnet we want to create</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621180609.png" class="kg-image" alt="21-June-25" loading="lazy" width="1514" height="823"></figure><p>Result, subnet <code>dev-subnet</code> created</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621180645.png" class="kg-image" alt="21-June-25" loading="lazy" width="1633" height="256"></figure><h3 id="nat-gateway">Nat Gateway</h3><blockquote>NAT Gateway is service you can use to private csubnet can connect to service outside your VPC but external services can&apos;t initiate a connection with those instances.</blockquote><p>When you create NAT gateway, you specisfy one of the following connectivity types :</p><ul><li><strong>Public</strong> : (Default) Instances in private subnets can connect to internet through a public NAT gateway, but the instances can&apos;t recive inbound connection from the internet</li><li><strong>Private</strong> : Instances in private subnets can connect to other VPCs on your on premises network through a private NAT gateway, but instances can&apos;t recive inbound connection other VPCs or the on-premises network. The defference with <strong>Public</strong> is You can route traffic from the NAT gateway through a transit gateway or a virtual private gateway. You can&apos;t associate an elastic IP address with a private NAT gateway. You can attach an internet gateway to a VPC with a private NAT gateway, but if you route traffic from the private NAT gateway to the internet gateway, the internet gateway drops the traffic.</li></ul><p>I want to create <em>Public</em> gateway, so first we need to <em>Associate ElasticIP</em> for launch <em>Nat Gateway</em> then. ElasticIP need to create because to NAT all private subnet to connect internet over ElasticIP.</p><p>Navigate to <code>VPC &gt; Elastic IPs &gt; Associate Elastic IP address</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621182931.png" class="kg-image" alt="21-June-25" loading="lazy" width="1870" height="595"></figure><p>Allocate Elastic IPs</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621183239.png" class="kg-image" alt="21-June-25" loading="lazy" width="1738" height="691"></figure><p>Then we begin to create NAT Gateway</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621183343.png" class="kg-image" alt="21-June-25" loading="lazy" width="1868" height="468"></figure><p>Configure public-subnet to use nat gateway</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621183724.png" class="kg-image" alt="21-June-25" loading="lazy" width="1724" height="671"></figure><p>Result,</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621185952.png" class="kg-image" alt="21-June-25" loading="lazy" width="1651" height="130"></figure><p>Edit route private subnets route table so that <code>0.0.0.0/0</code> point to Nat Gateway instead of IGW</p><p>Access to <code>VPC &gt; Route Tables &gt; Your private-subnet/VPC Route ID</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621185208.png" class="kg-image" alt="21-June-25" loading="lazy" width="1872" height="459"></figure><p>Edit Routes</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621185310.png" class="kg-image" alt="21-June-25" loading="lazy" width="1478" height="421"></figure><p>Add new route, select <code>0.0.0.0/0</code>, configure target to Nat Gatway, than Save</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621185355.png" class="kg-image" alt="21-June-25" loading="lazy" width="1452" height="188"></figure><p>Then result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621190041.png" class="kg-image" alt="21-June-25" loading="lazy" width="1457" height="431"></figure><h3 id="testing-1">Testing</h3><p>Create EC2 Instance, first attach to public-subnet, and then another to private-subnet. One instance on public-subnet attach elastic_IP to access SSH. And instance with private_subnet don&apos;t attach Elastic IP. Only private network.</p><p><strong>VM : pub-instance</strong></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621190625.png" class="kg-image" alt="21-June-25" loading="lazy" width="917" height="813"></figure><p><strong>VM : private-instance</strong></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621190820.png" class="kg-image" alt="21-June-25" loading="lazy" width="984" height="830"></figure><p>Access to <code>pub-instance</code>, testing curl to <code>https://ifconfig.me</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621205656.png" class="kg-image" alt="21-June-25" loading="lazy" width="1647" height="74"></figure><p>In <em>public-instance</em> we using <code>ElasticIP</code> from attached</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621205732.png" class="kg-image" alt="21-June-25" loading="lazy" width="414" height="41"></figure><p>To access <em>private-instance</em> we have to access <em>public_instance</em> then ssh <em>private-instance</em> using private-ip.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621205919.png" class="kg-image" alt="21-June-25" loading="lazy" width="1631" height="54"></figure><p>In <em>private-instance</em> we get publicIp from natGateway</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250621205947.png" class="kg-image" alt="21-June-25" loading="lazy" width="431" height="56"></figure>]]></content:encoded></item><item><title><![CDATA[20-June-25]]></title><description><![CDATA[Learn about basic VPC, subnet, IGW (Internet Gateway), SG (Security Group), and EC2 using AWS Free Tier.]]></description><link>https://twnb.nbtrisna.my.id/20-june-25/</link><guid isPermaLink="false">6856026edace4b00019e51d4</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Fri, 20 Jun 2025 00:53:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/967d5953-56e2-4a9f-b472-444faa0076cd.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="learn-aws-vpc">Learn AWS VPC</h3><img src="https://photoby.nbtrisna.my.id/967d5953-56e2-4a9f-b472-444faa0076cd.jpg" alt="20-June-25"><p><em>With amazon <code>virtual private cloud (VPC)</code>, you can launch aws resources in logicaly. isolated virtual network</em></p><p>Reference :</p><ul><li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html?ref=dev.nbtrisna.my.id">What is Amazon VPC?</a></li></ul><figure class="kg-card kg-image-card"><img src="https://docs.aws.amazon.com/images/vpc/latest/userguide/images/how-it-works.png" class="kg-image" alt="20-June-25" loading="lazy" width="521" height="311"></figure><p><strong>Explanation</strong> :</p><ul><li><strong>VPC</strong> : isolated private network, that closely resembles a traditional network that you&apos;d operate in own data center.</li><li><strong>Subnet</strong> : After create <code>VPC</code>, then we can set range of IP Address in your VPC. A subnet must reside in single <code>Availibilty Zone</code>. After youadd subnets, you can deploy AWS resources in VPC.</li><li><strong>Internet Gateway</strong> : Gateway connect vpc to another network, for example if you want to connect all resources in VPC to internet, you nedd a <code>internet-gateway</code></li><li><strong>Route Tables</strong> : Route table containts set of rules, called route. That are used to determine where network from your VPC is directed.</li></ul><h2 id="daily-quest-1-aws-free-tier-kickoff">Daily Quest #1: AWS Free Tier Kickoff</h2><blockquote>Learn about basic VPC, subnet, IGW (Internet Gateway), SG (Security Group), and EC2 using AWS Free Tier. Because i don&apos;t have AWS account, i need to <a href="https://signin.aws.amazon.com/signup?request_type=register&amp;ref=dev.nbtrisna.my.id">sign up</a> After login to account, makesure to enable 2FA for security reason.</blockquote><p>Reference :</p><ul><li>https://docs.aws.amazon.com/location/latest/developerguide/set-up.html</li></ul><h3 id="setup-iam-user">Setup IAM User</h3><blockquote>Instead login using <code>root_user</code>, AWS recomend to login using IAM user.</blockquote><p>I want to setup <em>IAM user</em> first, navigate to <code>Menu &gt; Security, Identitiy &amp; Compliance &gt; IAM</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620143242.png" class="kg-image" alt="20-June-25" loading="lazy" width="1154" height="971"></figure><p>in IAM Dashboard, navigate to <code>User &gt; Create User</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620143708.png" class="kg-image" alt="20-June-25" loading="lazy" width="1877" height="537"></figure><p>Then follow the step to create new IAM User. After all user details correct, click next</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620144230.png" class="kg-image" alt="20-June-25" loading="lazy" width="1708" height="866"></figure><p>I think i don&apos;t need to create new group for my aws account, so i prefer to attach policy directly to <code>AdministratorAccess</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620144403.png" class="kg-image" alt="20-June-25" loading="lazy" width="1716" height="661"></figure><p>After creating new IAM user, next login makesure to login using IAM user.</p><h3 id="aws-cli">AWS CLI</h3><blockquote>command line interface to manage all AWS Resources.</blockquote><p>Reference :</p><ul><li>https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</li></ul><p>Installing on macOs</p><pre><code class="language-sh">curl &quot;https://awscli.amazonaws.com/AWSCLIV2.pkg&quot; -o &quot;AWSCLIV2.pkg&quot;
sudo installer -pkg AWSCLIV2.pkg -target /

# Setup auto complate
vim ~/.zshrc
---
# Endfile
autoload bashcompinit &amp;&amp; bashcompinit
autoload -Uz compinit &amp;&amp; compinit
complete -C &apos;/usr/local/bin/aws_completer&apos; aws
</code></pre><p>Next create access key. Navigate to <code>IAM &gt; Users</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620153259.png" class="kg-image" alt="20-June-25" loading="lazy" width="1123" height="223"></figure><p>Then select your user, <code>Security Credentials &gt; Create Access Key</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620153509.png" class="kg-image" alt="20-June-25" loading="lazy" width="1536" height="831"></figure><p>Then select <code>Command Line Interface (CLI)</code> and makesure you backup <code>aws_access_key_id</code> and <code>aws_secret_access</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620153556.png" class="kg-image" alt="20-June-25" loading="lazy" width="1293" height="807"></figure><p>To login, exec below in terminal</p><pre><code class="language-sh">aws configure
</code></pre><blockquote>You need to input <code>aws_access_key_id</code> and <code>aws_secret_access</code> previously you generate.</blockquote><h3 id="setup-vpc">Setup VPC</h3><p>So first i want to create testing virtual machine, but before we provisioning VM (EC2), we need to setup VPC.</p><ol><li>Create VPC with <code>10.0.0.0/16</code></li></ol><blockquote>VPC is your private network. It&apos;s isolated from anything network or internet</blockquote><pre><code class="language-sh">VPC_ID=$(aws ec2 create-vpc --cidr-block 10.0.0.0/16 --query &apos;Vpc.VpcId&apos; --output text)

# check 
echo $VPC_ID
</code></pre><p>Result, Navigate to <code>VPC &gt; Your VPCs</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620155752.png" class="kg-image" alt="20-June-25" loading="lazy" width="1659" height="237"></figure><ol start="2"><li>Create public subnet</li></ol><blockquote>Subnet is your IP Address range can be used on EC2 Vm/Instances</blockquote><pre><code class="language-sh">SUBNET_ID=$(aws ec2 create-subnet --vpc-id $VPC_ID --cidr-block 10.0.1.0/24 --query &apos;Subnet.SubnetId&apos; --output text)

# check
echo $SUBNET_ID
</code></pre><p>Result, <code>VPC &gt; Subnets</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620155816.png" class="kg-image" alt="20-June-25" loading="lazy" width="1660" height="289"></figure><ol start="3"><li>Create &amp; Attach Interent gateway</li></ol><blockquote>Attach internet gateway to makesure your vpc network can accessing internet</blockquote><pre><code class="language-sh">IGW_ID=$(aws ec2 create-internet-gateway --query &apos;InternetGateway.InternetGatewayId&apos; --output text)

# Check 
echo $IGW_ID

# Attach to VPC
aws ec2 attach-internet-gateway --vpc-id $VPC_ID --internet-gateway-id $IGW_ID
</code></pre><p>Result, <code>VPC &gt; Internet Gateways</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620155846.png" class="kg-image" alt="20-June-25" loading="lazy" width="1656" height="194"></figure><ol start="4"><li>Setup Route Table &amp; Associate</li></ol><blockquote>Route table containts route, where network traffic from subnets/gateway directed</blockquote><pre><code class="language-sh">RTB_ID=$(aws ec2 create-route-table --vpc-id $VPC_ID --query &apos;RouteTable.RouteTableId&apos; --output text)

#check 
echo $RTB_ID

# next create route to internet, gateway using $IGW_ID
aws ec2 create-route --route-table-id $RTB_ID --destination-cidr-block 0.0.0.0/0 --gateway-id $IGW_ID

# next associate route table to subnet
aws ec2 associate-route-table --route-table-id $RTB_ID --subnet-id $SUBNET_ID
</code></pre><p>Check if route table to interet exists. <code>VPC &gt; Route Tables &gt; Select $RTB_ID</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620160607.png" class="kg-image" alt="20-June-25" loading="lazy" width="1852" height="551"></figure><p>Check if route table already associated to subnets <code>VPC &gt; Subnetes &gt; Select $SUB_ID &gt; Route Table</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620160902.png" class="kg-image" alt="20-June-25" loading="lazy" width="1824" height="751"></figure><ol start="5"><li>Create Security group &amp; Assign SSH Rule</li></ol><blockquote>A security group acts as a virtual firewall that controls the traffic for one or more instances.</blockquote><pre><code class="language-sh">SG_ID=$(aws ec2 create-security-group --group-name dev-sg --vpc-id $VPC_ID --query &apos;GroupId&apos; --output text --description &quot;Dev SG&quot;)

# check
$echo $SG_ID

# Assign security rule to allow ssh only from my public ip
aws ec2 authorize-security-group-ingress --group-id $SG_ID --protocol tcp --port 22 --cidr $(curl -s ifconfig.me)/32
</code></pre><p>Result,</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620161806.png" class="kg-image" alt="20-June-25" loading="lazy" width="910" height="333"></figure><p>Check if security group created <code>VPC &gt; Security Groups</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620161918.png" class="kg-image" alt="20-June-25" loading="lazy" width="1665" height="290"></figure><p>Check if security group rule created <code>VPC &gt; Security Groups &gt; $SG_ID</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620162013.png" class="kg-image" alt="20-June-25" loading="lazy" width="1511" height="447"></figure><ol start="6"><li>Launch EC2 Instance</li></ol><pre><code class="language-sh">INSTANCE_ID=$(aws ec2 run-instances --image-id ami-02c7683e4ca3ebf58 --instance-type t2.micro --subnet-id $SUBNET_ID --associate-public-ip-address --security-group-ids $SG_ID --key-name nb-key --query &apos;Instances[0].InstanceId&apos; --output text)

# check 
echo $INSTANCE_ID

# Get Public IP
PUB_IP=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query &apos;Reservations[0].Instances[0].PublicIpAddress&apos; --output text)

</code></pre><p>Result, navigate to <code>EC2 &gt; Instances</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620162830.png" class="kg-image" alt="20-June-25" loading="lazy" width="1875" height="443"></figure><p>Access instance</p><pre><code class="language-sh">ssh -i ~/.ssh/&lt;YourKeyPair&gt;.pem ubuntu@$PUB_IP 

</code></pre><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250620165529.png" class="kg-image" alt="20-June-25" loading="lazy" width="894" height="739"></figure><p><strong>Question</strong></p><ul><li>Mengapa kita perlu <strong>subnet publik + IGW</strong> daripada langsung &#x201C;internet-enabled&#x201D;?</li><li>Apa risiko membuka port 22 untuk publik dan bagaimana <strong>mitigasinya</strong> (hint: jump host)?</li></ul><p><strong>Answer</strong></p><ul><li>Karena secara default, VPC dalam aws itu tidak dapat mengakses keluar. Jadi disini kita perlu membuat sebuah subnet yang diarahkan routingnya ke gateway</li><li>Semua orang dapat mengakses, ini akan menjadikan kerentanan jika orang tidak bertanggung jawab bisa mengakses port SSH. Dan juga kadang SSH itu lawan di bruteforce</li></ul>]]></content:encoded></item><item><title><![CDATA[19-June-25]]></title><description><![CDATA[Create pipeline with lint & unit test, terraform, iaac testing (using terratest), Docker Build & Security Scan (Trivy), Deploy, Smoke Test & Metrics Scrape, Cleanup & Notification]]></description><link>https://twnb.nbtrisna.my.id/19-june-25/</link><guid isPermaLink="false">685601c7dace4b00019e51c2</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Thu, 19 Jun 2025 00:50:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/3a4489b7-1846-4855-8306-8a885e6ac289.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="archon-quest-realm-of-cicd">Archon Quest: Realm of CI/CD</h3><img src="https://photoby.nbtrisna.my.id/3a4489b7-1846-4855-8306-8a885e6ac289.jpg" alt="19-June-25"><p><em>Create pipeline with lint &amp; unit test, terraform, iaac testing (using <code>terratest</code>), Docker Build &amp; Security Scan (Trivy), Deploy, Smoke Test &amp; Metrics Scrape, Cleanup &amp; Notification</em></p><p><strong>Objective</strong></p><ul><li><strong>Lint &amp; Unit Test</strong> for both Node.js &amp; Go using a <strong>matrix</strong> strategy.</li><li><strong>Terraform Plan &amp; Apply</strong> on <code>main</code> branch (dev environment).</li><li><strong>Infrastructure Testing</strong> with Terratest.</li><li><strong>Docker Build &amp; Security Scan</strong> (Trivy).</li><li><strong>Deploy to Staging</strong> (auto) then <strong>Production</strong> (manual approvals).</li><li><strong>Smoke Tests &amp; Metrics Scrape</strong> (Prometheus).</li><li><strong>Cleanup &amp; Notification</strong>.</li></ul><h3 id="action">Action!</h3><p>Repository : https://github.com/ngurah-bagus-trisna/realm-of-cicd</p><ol><li>Define three environments in repo Settings: dev, staging, production (Add required reviewers in production)</li></ol><p>Create new repository first called <code>realm-of-cicd</code>, after that creating new environment by accesing <code>Settings &gt; Environment &gt; New Environmet</code></p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250619031254.png" class="kg-image" alt="19-June-25" loading="lazy" width="1838" height="1544"></figure><p>Result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250619031818.png" class="kg-image" alt="19-June-25" loading="lazy" width="1862" height="1022"></figure><ol start="2"><li>Setup linter for <code>nodejs-apps</code></li></ol><p>Reference :</p><ul><li>https://medium.com/opportunities-in-the-world-of-tech/how-i-set-up-ci-cd-for-a-node-js-app-with-eslint-jest-and-docker-hub-7d3cacf7add8</li></ul><pre><code class="language-sh">npm init -y  
npm install express

npm install --save-dev jest supertest
npx eslint --init
</code></pre><p>Create basic <code>helloWorld</code> node-js apps using express js.</p><pre><code class="language-js file:index.js">const express = require(&apos;express&apos;)
const app = express()

app.get(&apos;/&apos;, (req, res) =&gt; {
  res.send(&apos;Hello World!&apos;)
})

module.exports = app; 
</code></pre><pre><code class="language-js file:server.js">const app = require(&apos;./index&apos;);
const port = 8080;

app.listen(port, () =&gt; {
  console.log(`Example app listening on port ${port}`);
});
</code></pre><p>Create test unit using <code>jest</code></p><pre><code class="language-js file:tests/app.test.js">const request = require(&apos;supertest&apos;);
const app = require(&apos;../&apos;); 

describe(&apos;GET /&apos;, () =&gt; {
  it(&apos;responds with hello message&apos;, async () =&gt; {
    const res = await request(app).get(&apos;/&apos;);
    expect(res.statusCode).toEqual(200);
    expect(res.text).toContain(`Hello World!`);
  });
});
</code></pre><p>Configure <code>eslint.config.mjs</code> for lint <code>jest</code></p><pre><code class="language-js file:eslint.config.mjs">import js from &quot;@eslint/js&quot;;
import globals from &quot;globals&quot;;
import { defineConfig } from &quot;eslint/config&quot;;


export default defineConfig([
  { 
    files: [&quot;**/*.{js,mjs,cjs}&quot;], 
    plugins: { js }, 
    extends: [&quot;js/recommended&quot;] 
  },
  { 
    files: [&quot;**/*.{js,mjs,cjs}&quot;], 
    languageOptions: { 
      globals: globals.node 
    } 
  },
  { 
    files: [&quot;**/*.test.{js,mjs,cjs}&quot;], 
    languageOptions: { 
      globals: globals.jest  
    } 
  }
]);
</code></pre><p>Configure <code>package.json</code>, add script for <code>lint,test and dev</code></p><pre><code class="language-json file:package.json">  &quot;scripts&quot;: {
    &quot;lint&quot;: &quot;npx eslint .&quot;,
    &quot;test&quot;: &quot;jest&quot;,
    &quot;dev&quot;: &quot;node server.js&quot;
  },
</code></pre><p>Try to run first in terminal, makesure all passed.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250619183255.png" class="kg-image" alt="19-June-25" loading="lazy" width="422" height="346"></figure><ol start="3"><li>In quest, need to plan <code>main.tf</code> to create <code>hello.txt</code>.</li></ol><p>Create <code>main.tf</code></p><pre><code class="language-tf file:main.tf">terraform {
  required_providers {
    local = {
      source  = &quot;hashicorp/local&quot;
      version = &quot;~&gt; 2.0&quot;
    }
  }

  required_version = &quot;&gt;= 1.0&quot;
}

provider &quot;local&quot; {
  # No configuration needed for local provider
  
}

resource &quot;local_file&quot; &quot;hello_world&quot; {
  content  = &quot;Hello, OpenTofu!&quot;
  filename = &quot;${path.module}/test/hello.txt&quot;
  
}
</code></pre><blockquote>It will create a text file with the content <code>Hello, Opentofu!</code></blockquote><p>Create terratest to makesure terraform can apply <code>main.tf</code></p><pre><code class="language-go file:tests/test_infra.go">package test

import (
	&quot;testing&quot;

	&quot;github.com/gruntwork-io/terratest/modules/terraform&quot;
	&quot;github.com/stretchr/testify/assert&quot;
	&quot;os&quot;
)

func TestHelloFile(t *testing.T) {
	// retryable errors in terraform testing.
	terraformOptions := terraform.WithDefaultRetryableErrors(t, &amp;terraform.Options{
		TerraformDir: &quot;../&quot;,
	})

	defer terraform.Destroy(t, terraformOptions)

	terraform.InitAndApply(t, terraformOptions)
	
	content, err := os.ReadFile(&quot;hello.txt&quot;)
	assert.NoError(t, err)
	assert.Contains(t, string(content), &quot;Hello, OpenTofu!&quot;)
}	
</code></pre><blockquote>It will apply <code>main.tf</code>, and destroy after the check finished</blockquote><p>Init go modules &amp; install modules</p><pre><code class="language-sh">go mod init helo_test
go mod tidy

go test
</code></pre><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250619184125.png" class="kg-image" alt="19-June-25" loading="lazy" width="677" height="224"></figure><ol start="4"><li>Create github workflows <code>.github/workflows/archon-ci.yml</code></li></ol><pre><code class="language-yaml file:.github/workflows/archon-ci.yml">name: Archon CI
on:
  push:
    branches: [main]
jobs:
  lint-test:
    strategy:
      matrix:
        language: [node, go]
        node-version: [20, 18]
        go-version: [&quot;1.20&quot;, &quot;1.21&quot;]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        if: matrix.language == &apos;node&apos;
        uses: actions/setup-node@v2
        with:
          node-version: ${{ matrix.node-version }}       

      - name: Run lint and unit tests on nodejs
        if: matrix.language == &apos;node&apos;
        run: |
          npm install
          npm run lint
          npm run test

  IaC-apply:
    needs: lint-test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.1

      - name: Configure terraform plugin cache
        run: |
          echo &quot;TF_PLUGIN_CACHE_DIR=$HOME/.terraform.d/plugin-cache&quot; &gt;&gt;&quot;$GITHUB_ENV&quot;
          mkdir -p $HOME/.terraform.d/plugin-cache

      - name: Caching terraform providers
        uses: actions/cache@v4
        with:
          key: terraform-${{ runner.os }}-${{ hashFiles(&apos;**/.terraform.lock.hcl&apos;) }}
          path: |
            $HOME/.terraform.d/plugin-cache
          restore-keys: |
            terraform-${{ runner.os }}-

      - name: Apply terraform
        run: |
          terraform init 
          terraform apply -auto-approve

      - name: Export to artifact
        uses: actions/upload-artifact@v4
        with:
          name: Output files
          path: |
            tests/hello.txt
  
  build-image:
    needs: lint-test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
    
      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build docker image with layer cache
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ secrets.DOCKER_USERNAME }}/archon-image:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
        
      - name: Pull image
        run: |
          docker pull ${{ secrets.DOCKER_USERNAME }}/archon-image:latest

      - name: Scan docker image
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: ${{ secrets.DOCKER_USERNAME }}/archon-image:latest
          format: &apos;table&apos;
          severity: CRITICAL,HIGH
          ignore-unfixed: true
          exit-code: 1

      - name: Push docker image sha
        run: |
          # Add your docker push commands here, e.g.:
          docker tag ${{ secrets.DOCKER_USERNAME }}/archon-image:latest ${{ secrets.DOCKER_USERNAME }}/archon-image:${{ github.sha }}
          docker push ${{ secrets.DOCKER_USERNAME }}/archon-image:${{ github.sha }}

  deploy-development:
    needs: [build-image, IaC-apply]
    uses: ./.github/workflows/deploy.yaml
    with:
      environment: Development

  deploy-staging:
    needs: [deploy-development]
    uses: ./.github/workflows/deploy.yaml
    with:
      environment: Staging

  deploy-production:
    needs: [deploy-staging]
    uses: ./.github/workflows/deploy.yaml
    with:
      environment: Production

</code></pre><blockquote>Explanation : This workflow automates linting, testing, infrastructure deployment, Docker image building, and multi-environment deployments using a matrix strategy and job dependencies.</blockquote><p>Next create reusable workflow <code>deploy.yml</code></p><pre><code class="language-yaml file:.github/workflows/deploy.yml">name: Deploy Workflow
on:
  workflow_call:
    inputs:
      environment:
        description: &apos;The environment to deploy to&apos;
        required: true
        type: string
jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Deploy to ${{ inputs.environment }}
      run: |
        echo &quot;Deploy to ${{ inputs.environment }}&quot;
        docker run -d --name archon-${{ inputs.environment}} -p 8080:8080 ngurahbagustrisna/archon-image:latest
    
    - name: Wait for service to be ready
      run: |
        echo &quot;Waiting for service to be ready&quot;
        sleep 20  # Adjust the sleep time as necessary

    - name: Testing to hit using smoke tests on environment ${{ inputs.environment }}
      run: |
        echo &quot;Running smoke tests&quot;
        # Add your smoke test commands here, e.g.:
        chmod +x ./tests/smoke_test
        bash ./tests/smoke_test
        echo &quot;Finished deploy to ${{ inputs.environment }}&quot;
        docker rm -f archon-${{ inputs.environment}} || true
</code></pre><p>Push to github repository, and makesure all job passed.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250619184844.png" class="kg-image" alt="19-June-25" loading="lazy" width="1863" height="570"></figure>]]></content:encoded></item><item><title><![CDATA[18-June-25]]></title><description><![CDATA[Performance optimization in ci/cd pipeline focus on faster build & test for example using feature like caching, and pararel jobs]]></description><link>https://twnb.nbtrisna.my.id/18-june-25/</link><guid isPermaLink="false">68560188dace4b00019e51b3</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Wed, 18 Jun 2025 00:49:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/e7be8378-6c86-473e-b6fd-b78721e50f15.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="daily-quest-15-performance-optimization">Daily Quest #15: Performance Optimization</h3><img src="https://photoby.nbtrisna.my.id/e7be8378-6c86-473e-b6fd-b78721e50f15.jpg" alt="18-June-25"><p><em>Performance optimization in ci/cd pipeline focus on faster build &amp; test for example using feature like caching, and pararel jobs</em> </p><p><strong>Reference</strong> :</p><ul><li>https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/caching-dependencies-to-speed-up-workflows</li><li>https://github.com/docker/build-push-action</li></ul><blockquote><strong>Real world usecase :</strong> Instead to runing <code>terraform init</code> to re-download all provider, we can using <code>caching</code> in <code>.terraform/plugins</code>. An then <code>init</code> only check if update available.</blockquote><p><strong>Skenario</strong> : Using caching for terraform providers, node modules, docker layers and build pararel jobs in github actions for faster pipeline</p><ol><li>Create new workflow <code>.github/workflows/performance-opt.yml</code></li></ol><pre><code class="language-yaml file:.github/workflows/performance-opt.yml">name: Performance optimization
on:
  push:
    branches:
    - main
jobs:
  infra-cache:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Cache terraform providers
      uses: actions/cache@v4
      with:
        path: .terrafrom/plugins
        key: ${{ runner.os }}-terraform-${{ hashFiles(&apos;**/*.tf&apos;) }}
        restore-keys: |
          ${{ runner.os }}-terraform-
    
    - name: Setup terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.7.2

    - name: Terraform init &amp; Apply
      run: |
        terraform init 
        terraform plan -out=tfplan.out

  build:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: setup buildx
      uses: docker/setup-buildx-action@v3

    - name: Cache node modules
      uses: actions/cache@v4
      with:
        path: ~/.npm
        key: ${{ runner.os }}-node-${{ hashFiles(&apos;**/package-lock.json&apos;) }}
        restore-keys: |
          ${{ runner.os }}-node-
    - name: Build docker image with layer cache
      uses: docker/build-push-action@v4
      with:
        context: .
        file: Dockerfile
        push: false
        cache-from: type=gha
        cache-to: type=gha,mode=max
        tags: ci-cd-demo:latest
</code></pre><ol start="2"><li>Result running</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250619023125.png" class="kg-image" alt="18-June-25" loading="lazy" width="1856" height="1358"></figure><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250619023802.png" class="kg-image" alt="18-June-25" loading="lazy" width="1358" height="802"></figure><p><strong>Question</strong> :</p><ul><li>Bagaimana <strong>caching</strong> folder <code>.terraform/plugins</code> mempercepat <code>terraform init</code>?</li><li>Jelaskan cara kerja opsi <code>cache-from</code> dan <code>cache-to</code> di <code>docker/build-push-action</code>.</li></ul><p><strong>Answer</strong> :</p><ol><li>Dalam terraform init diperlukan caching agar tidak perlu mendownload depedency kembali ketika menjalankan terraform init. Ini akan mempersingkat waktu berjalanya workflow serta menghemat bandwith unduhan</li><li><code>cache-from</code> merupakan spesifik tempat dimana cache sebelumnya ditaruh. Dan cache-to adalah tempat dimana export build terbaru cache layer dimana mengizinkan untuk build dan menggunakan kembali cache tersebut.</li></ol>]]></content:encoded></item><item><title><![CDATA[17-June-25]]></title><description><![CDATA[Infrasturtucre testing in ci/cd makesure to running smoothly provisioning using terraform work realy well as expected]]></description><link>https://twnb.nbtrisna.my.id/17-june-25/</link><guid isPermaLink="false">68560158dace4b00019e51a6</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Tue, 17 Jun 2025 00:48:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/27177c79-139e-4cc6-866d-53a1a910f5b2.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="daily-quest-13-infrastructure-testing">Daily Quest #13: Infrastructure Testing</h3><img src="https://photoby.nbtrisna.my.id/27177c79-139e-4cc6-866d-53a1a910f5b2.jpg" alt="17-June-25"><p><em>Infrasturtucre testing in <code>ci/cd</code> makesure to running smoothly provisioning using terraform work realy well as expected.</em> Reference :</p><ul><li>https://terratest.gruntwork.io/</li><li>https://github.com/hashicorp/setup-terraform</li></ul><p>With <code>terratest</code> or kitchen-terraform you can write automate test with :</p><ul><li><code>terraform init </code>&amp; <code>apply</code> in temporarry workspace</li><li>Verify resource exsist and right configuration</li><li>Cleanup <code>destroy</code> after test complated</li></ul><blockquote><strong>Realworld Usecase :</strong> You have terraform module to create bucket in s3 storage. Terratest will apply that module, check that bucket created, and destroy resources.</blockquote><p><strong>Skenario</strong> : Integrate <code>terratest</code> to github actions for validate live infrasturcture post-apply and pre-merge</p><blockquote>makesure installing go first</blockquote><ol><li>Create directory <code>test/</code> and install go module ond <code>test/</code></li></ol><pre><code class="language-sh">mdkir test &amp;&amp; cd test
go mod init ci_cd_demo_test
got get github.com/gruntwork-io/terratest/modules/terraform
</code></pre><p>Create <code>text/terraform_hello_test.go</code></p><pre><code class="language-go">package test

import (
	&quot;testing&quot;

	&quot;github.com/gruntwork-io/terratest/modules/terraform&quot;
	&quot;github.com/stretchr/testify/assert&quot;
	&quot;io/ioutil&quot;
)

func TestHelloFile(t *testing.T) {
	// retryable errors in terraform testing.
	terraformOptions := terraform.WithDefaultRetryableErrors(t, &amp;terraform.Options{
		TerraformDir: &quot;../&quot;,
	})

	defer terraform.Destroy(t, terraformOptions)

	terraform.InitAndApply(t, terraformOptions)
	
	content, err := ioutil.ReadFile(&quot;hello.txt&quot;)
	assert.NoError(t, err)
	assert.Contains(t, string(content), &quot;Hello, OpenTofu!&quot;)
}	
</code></pre><p>Create <code>main.tf</code></p><pre><code class="language-terraform">terraform {
  required_providers {
    local = {
      source  = &quot;hashicorp/local&quot;
      version = &quot;~&gt; 2.0&quot;
    }
  }

  required_version = &quot;&gt;= 1.0&quot;
}

provider &quot;local&quot; {
  # No configuration needed for local provider
  
}

resource &quot;local_file&quot; &quot;hello_world&quot; {
  content  = &quot;Hello, OpenTofu!&quot;
  filename = &quot;${path.module}/test/hello.txt&quot;
  
}
</code></pre><p>Create workflow files <code>.github/workflows/infra-test.yml</code></p><pre><code class="language-yaml">name: Infrastructure as Code (IaC) testing
on:
  push:
jobs:
  test-iac:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Setup terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.7.3
      
      - name: Setup terratest
        uses: actions/setup-go@v5
        with:
          go-version: &apos;1.24&apos;
      
      - name: Run terratest
        working-directory: test
        run: |
          go test -v
</code></pre><ol start="2"><li>Push &amp; test passed</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250617111609.png" class="kg-image" alt="17-June-25" loading="lazy" width="1022" height="1006"></figure><p><strong>Answer</strong> :</p><ol><li>Instead using <code>teraform plan/validate</code> using terratest we can makes it easier to write automated tests for your infrastructure code. It provides a variety of helper functions and patterns for common infrastructure testing tasks</li><li>Kitchen terraform using <code>ruby</code> language. (jelaskan)</li></ol><p><strong>Refleksi Jawaban</strong></p><ol><li><strong>Mengapa Terratest vs <code>terraform plan</code>/<code>validate</code>?</strong><ul><li><strong><code>plan</code>/<code>validate</code></strong> hanya melakukan <em>static</em> pemeriksaan konfigurasi Terraform, tanpa menjalankan resource.</li><li><strong>Terratest</strong> melakukan <strong>live <code>init</code> &amp; <code>apply</code></strong>, lalu menjalankan pemeriksaan di runtime (misal membaca file, mengecek resource eksis), dan akhirnya <strong><code>destroy</code></strong>. Ini menangkap bug yang hanya muncul saat provisioning nyata&#x2014;misal kesalahan permission, path, atau ketergantungan environment.</li></ul></li><li><strong>Kenapa/Bagaimana Kitchen-Terraform menggunakan Ruby?</strong><ul><li><strong>Kitchen-Terraform</strong> adalah plugin untuk <strong>Test Kitchen</strong>, framework testing infrastruktur berbasis <strong>Ruby</strong>.</li><li>Kamu mendefinisikan <strong><code>platforms</code></strong>, <strong><code>provisioner</code></strong>, dan <code><strong>verifier</strong></code> (biasanya InSpec) di file <code>.kitchen.yml</code>.</li><li>Saat dijalankan, Test Kitchen (<code>kitchen converge</code>) akan apply Terraform, lalu InSpec (<code>kitchen verify</code>) menjalankan tes compliance/functional yang ditulis dalam Ruby DSL.</li><li>Ini cocok jika kamu tim yang sudah familier dengan ekosistem Ruby/Test Kitchen atau butuh <strong>InSpec</strong> untuk security/compliance testing.</li></ul></li></ol><hr><h2 id="daily-quest-14-cleanup-maintenance">Daily Quest #14: Cleanup &amp; Maintenance</h2><p><em>Makesure environment clean, build, and resource runner eficient</em> Reference :</p><ul><li>https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#scheduled-events</li><li>https://docs.docker.com/reference/cli/docker/system/prune/</li><li>https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/storing-and-sharing-data-from-a-workflow#setting-retention-period</li></ul><blockquote><strong>Real world case</strong> : Devops team running <code>self-hosted</code> runner only have 50GB disk, so in end-of-day, need to run <code>prune</code> image.</blockquote><p><strong>Skenario</strong> : Create workflows action to clean docker resource, volume and set artifact retention</p><ol><li>Create <code>cleanup-workflow.yml</code></li></ol><pre><code class="language-yaml">name: Cleanup &amp; maintenance
on:
  push:
    branches:
    - main
  schedule:
    - cron: &apos;0 0 * * *&apos; # Every Sunday at midnight
jobs:
  maintenance:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Cleanup old branches
        run: |
          docker system prune -a -f
          docker volume prune -f 
      - name: Cleanuup old terraform state files
        run: |
          find ${{ github.workspace }}/test -name &quot;*.tfstate&quot; -mtime +7 -delete
      - name: cleanup temporary workspace
        run: |
          rm -rf ${{ github.workspace }}/tmp/* || true
      - name: Generate report
        run: |
          echo &quot;Cleanup completed successfully on $(date)&quot; &gt; ${{ github.workspace }}/cleanup_report.txt
          cat ${{ github.workspace }}/cleanup_report.txt
      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: cleanup-report
          path: ${{ github.workspace }}/cleanup_report.txt


</code></pre><blockquote>Workflow run every push to branch main &amp; scheduled using cron every sunday midnight.</blockquote><ol start="2"><li>Push &amp; Result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250617122501.png" class="kg-image" alt="17-June-25" loading="lazy" width="1032" height="917"></figure><p><strong>Answer</strong> :</p><ol><li>Scheduling cleanup after off-peak hours is important to makesure not inffected production environment.</li><li>Retention-days make efficient storage and audit because deleted unused old resources.</li></ol><p><strong>&#x1F9E0; Refleksi Jawaban</strong></p><ol><li><strong>Mengapa schedule di off-peak hours penting?</strong><br>Menjalankan cleanup saat traffic rendah (off-peak) mengurangi risiko mengganggu job produksi dan menghindari bottleneck I/O di runner.</li><li><strong>Bagaimana <code>retention-days</code> bantu storage &amp; audit?</strong><br>Dengan membatasi umur artefak, kita mencegah penumpukan file usang&#x2014;menghemat storage dan memudahkan audit karena hanya artefak relevan yang tersimpan.</li></ol>]]></content:encoded></item><item><title><![CDATA[16-June-25]]></title><description><![CDATA[Observability & monitoring is key to understand about application and infrastructure healty in real-time]]></description><link>https://twnb.nbtrisna.my.id/16-june-25/</link><guid isPermaLink="false">68560121dace4b00019e5199</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Mon, 16 Jun 2025 00:47:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/45d95000-4c24-477d-9400-721733ecd596.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="daily-quest-12-observability-monitoring"><strong>Daily Quest #12: Observability &amp; Monitoring</strong></h3><img src="https://photoby.nbtrisna.my.id/45d95000-4c24-477d-9400-721733ecd596.jpg" alt="16-June-25"><p><em>Observability &amp; monitoring is key to understand about application and infrastructure healty in real-time</em></p><p>Reference :</p><ul><li>https://docs.github.com/en/actions/use-cases-and-examples/using-containerized-services/about-service-containers</li></ul><blockquote><strong>Real world usecase</strong> : Before deploying application in production, devops team scrape request latency and error rate in 30Second. if error rate &gt; 1% deployment canceled</blockquote><p>We use feature on github workflows called <code>service_containers</code>, you can use other tools using docker container that provide a simple and portable way for you to host services you might need to test or operate your application in a workflow.</p><blockquote><strong>Skenario</strong> : Integrate prometheus service in github actions, scrape basic metrics, and save snapshot using artefacts</blockquote><ol><li>Create folder <code>/monitoring</code> with <code>prometheus.yml</code> and <code>docker-compose.yaml</code></li></ol><pre><code class="language-yaml">global:
  scrape_interval: 5s
scrape_configs:
  - job_name: &apos;ci-cd-demo&apos;
    static_configs:
    - targets:
      - &apos;localhost:9090&apos;
</code></pre><pre><code class="language-yaml">name: Observability &amp; monitoring
on:
  push:
jobs:
  observe:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Running prometheus container
        run: |
          docker run -d \
            --name prometheus \
            -p 9090:9090 \
            -v ${{ github.workspace }}/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml \
            prom/prometheus:latest \
            --config.file=/etc/prometheus/prometheus.yml \
            --web.listen-address=:9090
      - name: Wait prometheus to be ready
        run: |
          echo &quot;Waiting for Prometheus to be ready...&quot;
          sleep 30s
          docker logs prometheus
          echo &quot;Prometheus should be ready now.&quot;

      - name: Run Prometheus
        run: |
          echo &quot;scrape metrics&quot;
          curl http://localhost:9090/metrics | head -n 10 &gt; prometheus_metrics.txt

      - name: Upload metrics to artifact
        uses: actions/upload-artifact@v4
        with:
          name: prometheus-metrics
          path: prometheus_metrics.txt 
</code></pre><ol start="2"><li>Push and see result</li></ol><figure class="kg-card kg-image-card"><img src="Pasted%20image%2020250616170324.png" class="kg-image" alt="16-June-25" loading="lazy"></figure><pre><code class="language-sh"># HELP go_gc_cycles_automatic_gc_cycles_total Count of completed GC cycles generated by the Go runtime. Sourced from /gc/cycles/automatic:gc-cycles.
# TYPE go_gc_cycles_automatic_gc_cycles_total counter
go_gc_cycles_automatic_gc_cycles_total 6
# HELP go_gc_cycles_forced_gc_cycles_total Count of completed GC cycles forced by the application. Sourced from /gc/cycles/forced:gc-cycles.
# TYPE go_gc_cycles_forced_gc_cycles_total counter
go_gc_cycles_forced_gc_cycles_total 0
# HELP go_gc_cycles_total_gc_cycles_total Count of all completed GC cycles. Sourced from /gc/cycles/total:gc-cycles.
# TYPE go_gc_cycles_total_gc_cycles_total counter
go_gc_cycles_total_gc_cycles_total 6
# HELP go_gc_duration_seconds A summary of the wall-time pause (stop-the-world) duration in garbage collection cycles.

</code></pre><p><strong>Answer</strong> (jelaskan dong):</p><ol><li>Scrape lebih unggul untuk mendapatkan data dari hal yang dimonitoring</li><li>Snapshot metrik dapat digunakan sebagai acuan dalam mendesign grafana dashboard</li></ol><hr>]]></content:encoded></item><item><title><![CDATA[15-June-25]]></title><description><![CDATA[Github actions, you can configure your workflows to run when spesific activity on github happens, schedule thime, or when event outside github occurs]]></description><link>https://twnb.nbtrisna.my.id/15-june-25/</link><guid isPermaLink="false">685600eadace4b00019e518c</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Sun, 15 Jun 2025 00:46:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/a106f5ea-2b7c-4554-ba5e-d41f467ce6f3.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="daily-quest-7-workflow-triggers"><strong>Daily Quest #7: Workflow Triggers</strong></h3><img src="https://photoby.nbtrisna.my.id/a106f5ea-2b7c-4554-ba5e-d41f467ce6f3.jpg" alt="15-June-25"><p><em>Github actions, you can configure your workflows to run when spesific activity on github happens, schedule thime, or when event outside github occurs</em> Workflow triggers are events that cause a worflow to run. Example you want to run job when somone push to main branch using <code>push</code> trigger, or <code>pull_request</code> when running job about validation to code before merge to main.</p><ol><li>Create <code>triggers-workflow.yml</code></li></ol><pre><code class="language-yaml">name: artifacts-workflows
on: 
  push:
  pull_request:
  schedule:
    - cron: &apos;0 0 * * *&apos; # Every day at midnight
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
        - name: Checkout code
          uses: actions/checkout@v4
        - name: Get event
          run: |
            echo &quot;Report at $(date)&quot; &gt; report.txt
            echo &quot;Triggered by ${{ github.event_name }}&quot; &gt;&gt; report.txt
        - name: Upload report
          uses: actions/upload-artifact@v4
          with:
            name: report
            path: report.txt
</code></pre><blockquote>explanation : This workflow running when push, pull_request, schedule on every midnight</blockquote><ol start="2"><li>Push, and result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615071106.png" class="kg-image" alt="15-June-25" loading="lazy" width="1950" height="1906"></figure><p>Workflow triggered by push to main branch</p><p><strong>Answer</strong> :</p><ol><li>Schedule akan dipilih ketika ingin membuat workflow berjalan pada waktu tertentu secara otomatis. Sedangkan <code>workflow_dispatch</code> dipilih jika ingin workflow tersebut berjalan manual</li><li>Hak akses harus diperhatikan ketika menggunakan manual trigger seperti <code>workflow_dispatch</code>. Mengantisipasi orang yang tidak bertanggung jawab menjalankan workflow</li></ol><hr><h2 id="daily-quest-8-parameterization-reuse"><strong>Daily Quest #8: Parameterization &amp; Reuse</strong></h2><p><em>Sometimes a complex workflow need value to adjust like environment name, script or flag build without duplicate a lot code in yaml workflow file. In github_actions, you can reuse wofklows so you and anyone. with access reusable workflow can call reusable workflow from another workflow</em></p><figure class="kg-card kg-image-card"><img src="https://docs.github.com/assets/cb-34427/images/help/actions/reusable-workflows-ci-cd.png" class="kg-image" alt="15-June-25" loading="lazy" width="1896" height="596"></figure><blockquote><strong>Real world case</strong> : If you have same deploy job for stg, QA, and production, you need one reusable workflow <code>deploy.yml</code>. And then call iit with input <code>env_stg</code>, or <code>env_prod</code> value.</blockquote><ol><li>Create <code>reusable.yml</code> workflow</li></ol><pre><code class="language-yaml">name: Reusable Template
on: 
  workflow_call:
    inputs:
      env_name:
        required: true
        type: string
      script_path:
        required: true
        type: string
jobs:
  run-script:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Echo parameters
        run: |
          echo &quot;Running script in environment: ${{ inputs.env_name }}&quot;
          echo &quot;Script path: ${{ inputs.script_path }}&quot;
      - name: Run script
        run: |
          bash ${{ inputs.script_path }}
</code></pre><ol start="2"><li>Create another workflow to call reusable, for this i called reusable to using <code>stg</code> path</li></ol><pre><code class="language-yaml">name: Call Reusable
on: 
  workflow_dispatch:

jobs:
  invoke:
    uses: ./.github/workflows/reusable.yaml
    with:
      env_name: staging
      script_path: ./script/staging.sh
</code></pre><ol start="3"><li>Push and see result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615074948.png" class="kg-image" alt="15-June-25" loading="lazy" width="1906" height="870"></figure><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615075012.png" class="kg-image" alt="15-June-25" loading="lazy" width="1924" height="1378"></figure><p><strong>Answer</strong> :</p><ol><li>Dengan menggunakan reusable workflow, kita dapat menggunakan satu workflow berkali&quot; tanpa menulis satu workflow di berbagai environment</li><li>Saya tidak akan menggunakan reusable workflows jika hanya menggunakan satu workflows saja.</li></ol><hr><h2 id="daily-quest-9-conditional-execution"><strong>Daily Quest #9: Conditional Execution</strong></h2><p><em>You can use expressions to programmatically set environment variables in workflows files and access contexts. Expression are commonly used with the conditional if keyword in a worflow file to determine whether a step should run. when an <code>if</code> conditional is <code>true</code>, the step will run</em></p><p>Reference :</p><ul><li>https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/evaluate-expressions-in-workflows-and-actions</li><li>https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#jobsjob_idif</li></ul><p>Using <code>if</code> expression in github action for running spesific job/step when an if conditional matches for saving resources and make pipeline more efficient.</p><ol><li>Create <code>conditional-workflow.yaml</code></li></ol><pre><code class="language-yaml">name: Conditional Workflow
on:
  push:
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Run tests
        if: github.ref == &apos;refs/heads/main&apos;
        run: |
          echo &quot;Running tests on main branch&quot;
      - name: Lint JS files
        if: contains(join(github.event.head_commit.modified, &apos;,&apos;), &apos;.js&apos;)
        run: |
          echo &quot;Linting JavaScript files&quot;
</code></pre><ol start="2"><li>Push and see result</li></ol><blockquote>Makesure lint js skipped (because in repository jo js file found)</blockquote><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615101508.png" class="kg-image" alt="15-June-25" loading="lazy" width="1922" height="986"></figure><p><strong>Answer</strong></p><ol><li>Kondisi di level jobs diperlukan ketika kondisi tersebut dilakukan untuk berbagai step kedepanya. (tolong jelasin lebih dibagian ini)</li><li>Kondisi terlalu komplex akan menyebabkan kesulitan saat membaca / maintain untuk yaml filesnya. Mitigasinya yaitu mengefisienkan js filesnya.</li></ol><p><strong>Reflection answer</strong></p><ol><li>Using <code>if</code> statement in jobs level if all job need to run or skipped based on condition. Example to run all lint or deploy job only on branch main. It will be clean than writing if in every job.</li><li>mitigation expresion to complex, first you can split condition to reusable workflows or composite actions to isolate the logic. Second you can using external script (JS/TS) to check condition,then call using <code>if</code> statement on workflows.</li></ol><hr><h2 id="daily-quest-10-security-scanning"><strong>Daily Quest #10: Security Scanning</strong></h2><p><em>Security is most important when we want to deploy our application to production. In pipeline devops, security scanning help to detect vulnerapility in source code or depedencies before deploy to production. Using tools like <code>trivy (container &amp; image scan)</code>, <code>sonarcube</code>, <code>dependabot</code>, can integrate to workflows. It&apos;s important so devops team can prevent security issues.</em> Reference :</p><ul><li>http://trivy.dev/latest/docs/</li><li>https://github.com/aquasecurity/trivy-action</li></ul><p><strong>Skenario</strong> : Creating actions to scan Dockerfile using trivy</p><ol><li>Create workflow <code>security-scan.yaml</code></li></ol><pre><code class="language-yaml">name: Security Scanning
on: 
  - push
jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
      - name: Build docker
        uses: docker/build-push-action@v2
      - name: Build docker image
        run: |
          echo &quot;Building Docker image...&quot;
          docker build -t ${{github.repository}}:latest .
          echo &quot;Docker image built successfully.&quot;
      - name: Run trivy scan scan
        uses: aquasecurity/trivy-action@0.28.0
        with:
          image-ref: ${{github.repository}}:latest
          format: table
          exit-code: 1
          vuln-type: os,library
          severity: CRITICAL,HIGH,MEDIUM
</code></pre><ol start="2"><li>Create simple dockerfile</li></ol><pre><code class="language-dockerfile">FROM ubuntu:20.04
</code></pre><ol start="3"><li>Push &amp; see results</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615231139.png" class="kg-image" alt="15-June-25" loading="lazy" width="768" height="708"></figure><blockquote>Found medium vulnerablity.</blockquote><p><strong>Answer</strong> :</p><ol><li>Set to exited code when found critical/high severity vulnerablitiy is for cancel workflows running further and notify dev have a vulnerability in image docker</li><li>We need to makesure severity beetwen production and development can passed for security</li></ol><hr><h2 id="daily-quest-11-concurrency-cancellation"><strong>Daily Quest #11: Concurrency &amp; Cancellation</strong></h2><p><em>In ci/cd, concurrency makesure only one workflow run in group session (like branch/or workflow.</em> Reference :</p><ul><li>https://docs.github.com/en/actions/writing-workflows/workflow-syntax-for-github-actions#concurrency</li></ul><blockquote><strong>real world usecase</strong> : When all dev repetitivly commit and push hotfix to main branch, if another job or workflow using the same concurrency group in the repository is in progress, the queued job or workflow will be pending</blockquote><p><strong>Skenario</strong> : Create concurency workflow, only run when latest update. And when another concurency running, just cancel. Only running latest push</p><ol><li>Create <code>concurrency-workflow.yaml</code></li></ol><pre><code class="language-yaml">name: Concurency workflows
on: 
  - push
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true
jobs:
  concurrency-workflow:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Run concurrency workflow
        run: echo &quot;This is a concurrency workflow that will run only once per branch.&quot;

      - name: Show run number
        run: |
          echo &quot;Run number: ${{ github.run_number }} on ref ${{ github.ref }}&quot;
      - name: Simulate work
        run: |
          echo &quot;Simulating work...&quot;
          sleep 30
          echo &quot;Work done!&quot; 
        
</code></pre><ol start="2"><li>Push, &amp; result.</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615235146.png" class="kg-image" alt="15-June-25" loading="lazy" width="1105" height="454"></figure><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615235151.png" class="kg-image" alt="15-June-25" loading="lazy" width="1128" height="712"></figure><blockquote>Workflow canceled because have higher priority on latest update on repository.</blockquote><p><strong>Answer</strong></p><ol><li><code>group</code> and <code>cancel-in-progress</code> can cancle running workflow when have a condiiton same group running workflow with higher priority (like latest update on repository)</li><li>Skenario when need to push another version (Tolong jelaskan)</li></ol>]]></content:encoded></item><item><title><![CDATA[14-June-25]]></title><description><![CDATA[Workflow running jobs parrarel by default. To run jobs sequentially, you need to define other jobs using the jobs.<job_id>.needs keyword]]></description><link>https://twnb.nbtrisna.my.id/14-june-25/</link><guid isPermaLink="false">685600a8dace4b00019e517f</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Sat, 14 Jun 2025 00:45:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/520ce5bd-636b-4bec-a836-50fd5932b33b.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="daily-quest-5-job-dependencies"><strong>Daily Quest #5: Job Dependencies</strong></h3><img src="https://photoby.nbtrisna.my.id/520ce5bd-636b-4bec-a836-50fd5932b33b.jpg" alt="14-June-25"><p><em>Workflow running jobs parrarel by default. To run jobs sequentially, you need to define other jobs using the <code>jobs.&lt;job_id&gt;.needs</code> keyword</em></p><ol><li>Add new workflows <code>needs-workflows.yaml</code></li></ol><pre><code class="language-yaml">name: Needs worflows
on: 
  workflow_dispatch:
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
        - name: Checkout code
          uses: actions/checkout@v2
        - name: Build stage
          run: |
            echo &quot;Hello from CI/CD!&quot;
            echo &quot;Running on build job&quot;
  test:
    runs-on: ubuntu-latest
    needs: build
    steps:
        - name: Greeting from variable
          run: echo &quot;Hello from ${{ env.GREETING }}&quot;
        - name: Echo secret
          run: echo &quot;Secret is ${{ secrets.SECRET_MSG }}&quot;
        - name: Test stage
          run: |
            echo &quot;Running on test job&quot;
            echo &quot;This job depends on the build job&quot;
</code></pre><p>Push, and result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250614042555.png" class="kg-image" alt="14-June-25" loading="lazy" width="898" height="870"></figure><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250614042607.png" class="kg-image" alt="14-June-25" loading="lazy" width="893" height="439"></figure><p><strong>Answer</strong> :</p><ol><li>Dengan menjalankan job satu persatu, workflows dapat membuat efisien task hanya di satu jobs terlebih dahulu.</li><li>Job akan gagal dan di skip ke next jobs (jika memenuhi peraturan).</li></ol><h2 id="daily-quest-6-artifact-management"><strong>Daily Quest #6: Artifact Management</strong></h2><p>Referensi : https://docs.github.com/actions/using-workflows/storing-workflow-data-as-artifacts <em>Artifacts allow you to persist data after job has completed, and share that data with another job in the same workflows.</em> Artifacts is file/collection of files produced during workflow run. For example, you can use artifacts to save your build &amp; test output after a workflow run has ended. <strong>How to use</strong> You can use <code>actions/upload-artifact</code> to save files on artifacts.</p><pre><code class="language-yaml">      - name: Archive code coverage results
        uses: actions/upload-artifact@v4
        with:
          name: code-coverage-report
          path: output/test/code-coverage.html
</code></pre><p>Explanation :</p><ul><li><code>name</code> : name artifacts</li><li><code>path</code> : path file for upload to artifacts</li></ul><p>To download file from artifacts, you can use <code>actions/download-artifact</code> action to download artifacts that were previously uploaded in the same workflow run.</p><pre><code class="language-yaml">- name: Download a single artifact
  uses: actions/download-artifact@v4
  with:
    name: my-artifact
</code></pre><p>Explanation :</p><ul><li><code>name</code> : name previously uploaded artifacts</li></ul><ol><li>Create new workflows <code>artifact-workflow.yml</code></li></ol><pre><code class="language-yaml">name: artifacts-workflows
on: 
  push:
    branches:
      - main
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
        - name: Checkout code
          uses: actions/checkout@v4
        - name: Create report
          run: |
            echo &quot;Report at $date&quot; &gt; report.txt
        - name: upload artifacts
          uses: actions/upload-artifact@v4
          with:
            name: my-report
            path: report.txt
  consume:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: consume artifacts
        uses: actions/download-artifact@v4
        with:
          name: my-report
      - name: show report
        run: |
          cat report.txt
          echo &quot;Report consumed successfully&quot;
</code></pre><p>Result :</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250614205521.png" class="kg-image" alt="14-June-25" loading="lazy" width="1104" height="1002"></figure><ol><li>Saya akan menggunakan artifact ketika ingin export hasil dari job yang sudah running. Sedangkan cache hanya untuk depedencies aplikasi yang digunakan berulang setiap kali jobs running</li><li>Artifacts membantu untuk menggungah/mengunduh file di tiap job yang berjalan.</li></ol>]]></content:encoded></item><item><title><![CDATA[13-June-25]]></title><description><![CDATA[In CI/CD world, environment variables and secret is key to storing value without writing directly to yaml files. It's possible to manage credentials, versioning, and configure build according to the needs]]></description><link>https://twnb.nbtrisna.my.id/13-june-25/</link><guid isPermaLink="false">6855ffbddace4b00019e516f</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Fri, 13 Jun 2025 00:43:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/14e99474-f094-4fa0-b62c-f2998351d5dc.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="daily-quest-2-secrets-environment-variables"><strong>Daily Quest #2: Secrets &amp; Environment Variables</strong></h3><img src="https://photoby.nbtrisna.my.id/14e99474-f094-4fa0-b62c-f2998351d5dc.jpg" alt="13-June-25"><p><em>In CI/CD world, environment variables and secret is key to storing value without writing directly to yaml files. It&apos;s possible to manage credentials, versioning, and configure build according to the needs</em></p><p><strong>Skenario</strong> : Understand how to use <code>env</code> and <code>secret</code> in github actions for flexible and secure workflow.</p><ol><li>In existing repository, reconfigure <code>first-workflows.yaml</code>.</li></ol><pre><code class="language-yaml">name: Hello CI
on: 
    - push
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
        - name: Checkout code
          uses: actions/checkout@v2
        - name: say hello
          run: echo &quot;Hello from CI/CD!&quot;
        - name: Greeting from variable
          run: echo &quot;Hello from ${{ env.GREETING }}&quot;
        - name: Echo secret
          run: echo &quot;Secret is ${{ secrets.SECRET_MSG }}&quot;
</code></pre><ol start="2"><li>Create secrets on github. Navigate to <code>Settings &gt; Secrets and variables &gt; Actions</code></li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615135106.png" class="kg-image" alt="13-June-25" loading="lazy" width="961" height="929"></figure><p>2. Create secrets &amp; variable, makesure name of variable/secret correct</p><ol start="3"><li>Result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615135035.png" class="kg-image" alt="13-June-25" loading="lazy" width="955" height="907"></figure><p>Answer :</p><ol><li>Penggunaan secrets digunakan untuk store data penting seperti password. Ketika digunakan dalam workflows, akan di samarkan dengan <code>**</code>. Untuk variable, biasanya digunakan untuk menyimpan config, dll. Yang sifatnya general</li><li>Github workflow memask secret agar isi dari secret tidak terlihat. Karena biasanya data yang tersimpan dalam secrets adalah data rahasia.</li></ol><h2 id="daily-quest-3-matrix-mastery"><strong>Daily Quest #3: Matrix Mastery</strong></h2><p><em>In github workflows, we can use <code>matrix strategies</code> for create multiple job runs using a single jobs variable definition like job running pararalel simultaneously. For example like you can configure your ci to run build in 3 different os/arch</em></p><p>Referensi : https://docs.github.com/en/actions/writing-workflows/choosing-what-your-workflow-does/running-variations-of-jobs-in-a-workflow</p><p>To define <em>matrix_strategy</em> simply put in <code>jobs.&lt;job_id&gt;.strategy.matrix</code> with value array.</p><pre><code class="language-yaml">jobs:
	example_matrix:
		strategy:
			matrix:
				os: [ubuntu-latest, selfhosted]
				version: [10, 11, 12]
</code></pre><p>By default, GitHub will maximize the number of jobs run in parallel depending on runner availability. The order of the variables in the matrix determines the order in which the jobs are created. The first variable you define will be the first job that is created in your workflow run. For example, the above matrix will create the jobs in the following order:</p><pre><code>{version: 10, os: ubuntu-latest}
{version: 10, os: windows-latest}
{version: 12, os: ubuntu-latest}
{version: 12, os: windows-latest}
{version: 14, os: ubuntu-latest}
{version: 14, os: windows-latest}
</code></pre><h4 id="single-dimension-matrix">Single-dimension matrix</h4><pre><code class="language-yaml">jobs:
  example_matrix:
    strategy:
      matrix:
        version: [10, 12, 14]
    steps:
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.version }}

</code></pre><p>Explanation : For example, the following workflow defines the variable version with the values [10, 12, 14]. The workflow will run three jobs, one for each value in the variable. Each job will access the version value through the matrix.version context and pass the value as node-version to the actions/setup-node action.</p><h4 id="multi-dimension-matrix">Multi-dimension matrix</h4><pre><code class="language-yaml">jobs:
  example_matrix:
    strategy:
      matrix:
        os: [ubuntu-22.04, ubuntu-20.04]
        version: [10, 12, 14]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.version }}
</code></pre><h3 id="quest">Quest</h3><ol><li>Create new job called <code>matrix-workflow.yaml</code>. Using <code>matrix-strategy</code> to run jobs in different os</li></ol><pre><code class="language-yaml">name: Hello CI
on: 
    - push
jobs:
  build:
    strategy:
      matrix: 
        os: [ubuntu-latest, windows-latest, macos-latest]
    runs-on: ${{ matrix.os }}
    steps:
        - name: Checkout code
          uses: actions/checkout@v2
        - name: say hello
          run: echo &quot;Hello from CI/CD!&quot;
        - name: Greeting from variable
          run: echo &quot;Hello from ${{ env.GREETING }}&quot;
        - name: Echo secret
          run: echo &quot;Secret is ${{ secrets.SECRET_MSG }}&quot;
        - name: Display OS
          run: echo &quot;Running on ${{ matrix.os }}&quot;
</code></pre><ol start="2"><li>Push, and see result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615135116.png" class="kg-image" alt="13-June-25" loading="lazy" width="961" height="842"></figure><p><strong>Answer</strong> :</p><ol><li>Kelebihan utama menggunakan matrix yaitu dapat mempersingkat dan membuat efiisien dari file workflow. Ketika ada perubahan besar di yaml workflow, devops hanya mengubah satu job. Tanpa mengulangi edit yang lain</li><li>Matrix dapat menjadi overkill karena dapat menjalankan secara paralel setiap jobs sesuai dengan variable yang di definisikan dalam array. Untuk membatasinya, kita dapat membatasi sesuai dengan kebutuhan(? coba koreksi)</li></ol><h2 id="daily-quest-4-dependency-caching"><strong>Daily Quest #4: Dependency Caching</strong></h2><p>Refrensi : https://docs.github.com/actions/using-workflows/caching-dependencies-to-speed-up-workflows <em>To make workflows faster and efficient, you can cache for depedencies and other commonly reused file.</em> Job in github-hosted runners start a clean runner image, and must download depedencies each time. Causing incerased network utilizaiton, longiger runtime, and incerase cost. To cache depedencies for a job, you can use <code>cache-action</code>. The action create and restores a cache identified by a unique key.</p><p><strong>Artifacts vs caching</strong></p><ul><li>Use caching when you want to reuse files that don&apos;t change often between jobs/workflows runs. Such a depedencies from package management system (npm, dll)</li><li>Using artifacts when you want to save files produce by a job to view after a workflow run has ended. Such a built binaries/logs.</li></ul><p><strong>Using a <code>cache</code> actions</strong></p><ul><li>First search for excat match to your provided <code>key</code></li><li>if no excact match, it will search for partial matches of the <code>key</code></li><li>if there is still no match found, and you&apos;ve provided <code>restore-keys</code>, these keys wil be checked sequenial for partial matches. <strong>Input parameters for the cache action</strong></li><li><code>key</code> : Using to search for a cache. It can any combination of variables, context values, static strings, and function.</li><li><code>path</code> : The path of the runner to cache/restore</li><li><code>restore-keys</code> : Alternative restore keys.</li></ul><p>Is quite hard to understand to using <code>key</code> and <code>restore-key</code>. So first if key not match -&gt; search to restore-keys to list what to restore on path.</p><pre><code class="language-sh">       +-------------------------+
       |  Start cache step      |
       +-------------------------+
                 |
                 v
      +---------------------------+
      |  Key match with cache?   |
      +---------------------------+
           |              |
          Yes            No
           |              |
           v              v
  Restore full         Try restore with
    cache              restore-keys prefix
           |              |
         (Hit)         (Partial hit / miss)
           |              |
           v              v
    Run job (npm install, dll)
           |
           v
  +------------------------------+
  |  Save cache at end?         |
  +------------------------------+
           |
  Only if &#x274C; cache miss
</code></pre><p>Cache : only run if cache miss.</p><p><strong>Skenario</strong> : add cache before installing deedencies</p><ol><li>Edit <code>cache-workflow.yml</code></li></ol><pre><code class="language-yaml file:cache-workflow.yml">name: Cache Workflows
on: 
  push:
    branches:
      - main
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
        - name: Checkout code
          uses: actions/checkout@v2
        - name: say hello
          run: echo &quot;Hello from CI/CD!&quot;
        - name: Greeting from variable
          run: echo &quot;Hello from ${{ env.GREETING }}&quot;
        - name: Echo secret
          run: echo &quot;Secret is ${{ secrets.SECRET_MSG }}&quot;
        - name: Cache node modules 
          uses: actions/cache@v4
          with:
            path: ~/.npm
            key: ${{ runner.os }}-node-${{ hashFiles(&apos;**/package-lock.json&apos;) }}
            restore-keys: |
              ${{ runner.os }}-node-
        - name: Install dependencies
          run: npm install
        - if: ${{ steps.cache-npm.outputs.cache-hit != &apos;true&apos; }}
          name: List the state of node modules
          continue-on-error: true
          run: npm list
</code></pre><ol start="2"><li>Push, and result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250615135122.png" class="kg-image" alt="13-June-25" loading="lazy" width="961" height="842"></figure><p><strong>Answer</strong></p><ol><li>Key and restore-keys akan melakukan pengecekan, jika key match maka akan di restore seluruh yang ada di path. Jika tidak, maka akan melakukan general check dengan restore-keys. Jika match maka restore, jika tidak maka hanya akan di cache dan tidak restore</li><li>Tidak menggunakan caching ketika workflow/job yang dijalankan tidak memerlukan depedencies berulang.</li></ol>]]></content:encoded></item><item><title><![CDATA[12-June-25]]></title><description><![CDATA[OpenTofu is fork from opensource terraform tool for infrastructure as a code. This tool allow to define infrastructure to human readable file `.tf` so you can manage consistant infrastructure throught lifesycle]]></description><link>https://twnb.nbtrisna.my.id/12-june-25/</link><guid isPermaLink="false">6855fe39dace4b00019e515d</guid><category><![CDATA[daily-notes]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Thu, 12 Jun 2025 00:36:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/4bd88ec4-2504-4d83-8459-5d7f9ccd8a3b.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="learn-opentofu">Learn Opentofu</h3><img src="https://photoby.nbtrisna.my.id/4bd88ec4-2504-4d83-8459-5d7f9ccd8a3b.jpg" alt="12-June-25"><p>Referensi : https://opentofu.org/docs/ <em>OpenTofu is fork from opensource terraform tool for infrastructure as a code. This tool allow to define infrastructure to human readable file <code>.tf</code> so you can manage consistant infrastructure throught lifesycle.</em></p><p>In first daily quest, we learn to create simple file <code>hello.txt</code> using <code>local</code> provider.</p><p>OpenTofu relies on plugins called providers to interact with cloud providers, SaaS providers, and other APIs.</p><p><em>Resources</em>&#xA0;are the most important element in the OpenTofu language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.</p><p>Alur init -&gt; plan -&gt; apply diperlukan agar sebelum infrastruktur dijalankan, ops bisa cek.</p><h4 id="hello-world">Hello world</h4><ol><li>Makesure tofu installed</li><li>Create <code>main.tf</code> file</li></ol><pre><code class="language-tf file:main.tf">terraform {
  required_providers {
    local = {
      source  = &quot;hashicorp/local&quot;
      version = &quot;~&gt; 2.0&quot;
    }
  }

  required_version = &quot;&gt;= 1.0&quot;
}

provider &quot;local&quot; {
  # No configuration needed for local provider
  
}

resource &quot;local_file&quot; &quot;hello_world&quot; {
  content  = &quot;Hello, Opentofu!&quot;
  filename = &quot;${path.module}/hello.txt&quot;
  
}
</code></pre><ol start="3"><li>Setup provider &amp; apply</li></ol><pre><code class="language-sh">tofu init
tofu plan
tofu apply
</code></pre><p>Result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612085123.png" class="kg-image" alt="12-June-25" loading="lazy" width="1250" height="440"></figure><h4 id="plan-inception">Plan Inception</h4><p><em>After init, opentofu create plan without apply anything about main.tf. Its crucial for developer to review things you need to create.</em></p><p><strong>Try</strong> : skenario we change value content to <code>Hello, Opentofu! V2</code></p><ol><li>Run tofu plan with out for check changes before we edit <code>main.tf</code></li></ol><pre><code class="language-sh">tofu plan -out=plan2.out
---

No changes. Your infrastructure matches the configuration.

OpenTofu has compared your real infrastructure against your configuration and
found no differences, so no changes are needed.

---
# print to text
tofu show plan.out &gt; before.txt
</code></pre><ol start="2"><li>Edit <code>main.tf</code></li></ol><pre><code class="language-sh file:main.tf"># before

resource &quot;local_file&quot; &quot;hello_world&quot; {
  content  = &quot;Hello, Opentofu!&quot;
  filename = &quot;${path.module}/hello.txt&quot;
}

# after

resource &quot;local_file&quot; &quot;hello_world&quot; {
  content  = &quot;Hello, Opentofu! V2&quot;
  filename = &quot;${path.module}/hello.txt&quot;
}
</code></pre><ol start="3"><li>Plan again with different output file</li></ol><pre><code class="language-sh">tofu plan -out=plan2.out
---
local_file.hello_world: Refreshing state... [id=38191fd8e74a432bf7ffc42ceee0bb4fb06e658d]

OpenTofu used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
-/+ destroy and then create replacement

OpenTofu will perform the following actions:

  # local_file.hello_world must be replaced
-/+ resource &quot;local_file&quot; &quot;hello_world&quot; {
      ~ content              = &quot;Hello, Opentofu!&quot; -&gt; &quot;Hello, Opentofu! V2&quot; # forces replacement
      ~ content_base64sha256 = &quot;lL/QFDa0wcJiPe1okWxphgQ0/2kax3o2shzTdVu8tTs=&quot; -&gt; (known after apply)
      ~ content_base64sha512 = &quot;6JPryukFlv/rFOyC6tMLea+tQ2ZTJZxFsL57CYa3rXC/a28XaB3JmPerpMUSfWS29PyOr9lyCP2nxmIyqTnHpA==&quot; -&gt; (known after apply)
      ~ content_md5          = &quot;4c66612d6cb3994e929b4ed74c1b7290&quot; -&gt; (known after apply)
      ~ content_sha1         = &quot;38191fd8e74a432bf7ffc42ceee0bb4fb06e658d&quot; -&gt; (known after apply)
      ~ content_sha256       = &quot;94bfd01436b4c1c2623ded68916c69860434ff691ac77a36b21cd3755bbcb53b&quot; -&gt; (known after apply)
      ~ content_sha512       = &quot;e893ebcae90596ffeb14ec82ead30b79afad436653259c45b0be7b0986b7ad70bf6b6f17681dc998f7aba4c5127d64b6f4fc8eafd97208fda7c66232a939c7a4&quot; -&gt; (known after apply)
      ~ id                   = &quot;38191fd8e74a432bf7ffc42ceee0bb4fb06e658d&quot; -&gt; (known after apply)
        # (3 unchanged attributes hidden)
    }

Plan: 1 to add, 0 to change, 1 to destroy.

&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;&#x2500;

Saved the plan to: plan2.out

To perform exactly these actions, run the following command to apply:
    tofu apply &quot;plan2.out&quot;
</code></pre><ol start="5"><li>Check difference between before.txt and after.txt</li></ol><pre><code class="language-sh">diff before.txt after.txt
</code></pre><p>Result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612110838.png" class="kg-image" alt="12-June-25" loading="lazy" width="2852" height="1068"></figure><h4 id="apply">Apply</h4><p>*After reviewing on plan-stage, next step is apply resources using command <code>tofu apply</code> *</p><p><strong>Skenario</strong> : Apply <code>plan2.out</code></p><blockquote>In help <code>apply</code> mean Creates or updates infrastructure according to OpenTofu configuration files in the current directory.</blockquote><p>By default, OpenTofu will generate a new plan and present it for your approval before taking any action. You can optionally provide a plan file created by a previous call to &quot;tofu plan&quot;, in which case OpenTofu will take the actions described in that plan without any confirmation prompt.</p><ol><li>Apply <code>plan2.out</code></li></ol><pre><code class="language-sh">tofu apply plan2.out
</code></pre><ol start="2"><li>Result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612111149.png" class="kg-image" alt="12-June-25" loading="lazy" width="1810" height="1282"></figure><ol start="4"><li><em>Destroy</em> for deleting all resources</li></ol><pre><code class="language-sh">tofu destroy -auto-approve # !IMPORTANT, dont use in production
</code></pre><p>Result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612111255.png" class="kg-image" alt="12-June-25" loading="lazy" width="2528" height="1184"></figure><p><strong>Answer</strong></p><ol><li>Perbedaan apply plan2.out dengan auto approve adalah ketika menggunakan plan2 adalah kondisi ketika tofu itu sudah di cek dalam stage plan. Sedangakan dengfan auto approve, resource langsung dibuat sesuai dengan file <code>main.tf</code></li><li>Destroy merupakan hal kritikal yang wajib dihindari apalagi dengan <code>-auto-approve</code> yang langsung menghapus resource tanpa adanya konfirmasi. Diwajibkan dengan mengtiadakan <code>-auto-approve</code></li></ol><p>https://github.com</p><h3 id="bos-quest">Bos quest</h3><p><strong>Archon Quest: The Forge of Foundations</strong></p><ol><li>Buat <code>main.tf</code></li></ol><pre><code class="language-tf file:main.tf">terraform {
  required_providers {
    local = {
      source  = &quot;hashicorp/local&quot;
      version = &quot;~&gt; 2.0&quot;
    }
  }

  required_version = &quot;&gt;= 1.0&quot;
}

provider &quot;local&quot; {
  # No configuration needed for local provider
  
}

resource &quot;local_file&quot; &quot;hello_world&quot; {
  content  = &quot;Hello, OpenTofu! V2&quot;
  filename = &quot;${path.module}/otf-local/hello.txt&quot;
  
}
</code></pre><ol start="2"><li>Buat <code>.github/workflows/ci.yaml</code></li></ol><pre><code class="language-yaml file:.github/workflows/ci.yaml">name: Apply Tofu to create hello.txt
on:
  push:
    branches:
      - main
jobs:
  apply:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Tofu
        uses: opentofu/setup-opentofu@v1

      - name: Apply Tofu
        run: |
          tofu init
          tofu plan
          tofu apply -auto-approve

      - name: Verify otf-local/hello.txt value
        run: |
          if ! grep -q &quot;Hello, OpenTofu! V2&quot; otf-local/hello.txt; then  
            echo &quot;Validation failed!&quot; &amp;&amp; exit 1  
          fi  

      - name: Upload hello.txt artifact
        uses: actions/upload-artifact@v4
        with:
          name: hello.txt
          path: otf-local/hello.txt
</code></pre><ol start="3"><li>Pastikan repository sudah di setup + push ke main branch `</li></ol><pre><code class="language-sh">git push -u origin main
</code></pre><ol start="4"><li>Result</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612114614.png" class="kg-image" alt="12-June-25" loading="lazy" width="3262" height="1294"></figure><p>Repository : https://github.com/ngurah-bagus-trisna/boss-land-of-iac</p><h3 id="%F0%9F%8C%9F-side-quest-shell-scripting-sprint">&#x1F31F; Side Quest: Shell Scripting Sprint</h3><p>Refrensi : https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html</p><ol><li>Buat <code>backup.sh</code></li></ol><pre><code class="language-sh file:backup.sh">#!/usr/bin/env bash

SRC_DIR=&quot;$HOME/my-data&quot;
DEST_DIR=&quot;$HOME/backup-$(date +%Y%m%d)&quot;

if [ ! -d &quot;$SRC_DIR&quot; ]; then
    echo &quot;Source directory $SRC_DIR does not exist.&quot;
    exit 1
else 
    echo &quot;Backing up $SRC_DIR to $DEST_DIR&quot;
    mkdir -p &quot;$DEST_DIR&quot;
    cp -r &quot;$SRC_DIR/&quot;* &quot;$DEST_DIR/&quot;
    if [ $? -eq 0 ]; then
        echo &quot;Backup completed successfully.&quot;
    else
        echo &quot;Backup failed.&quot;
        exit 1
    fi
fi
</code></pre><ol start="2"><li>Jadikan executable</li></ol><pre><code class="language-sh">chmod +x backup.sh
</code></pre><ol start="3"><li>Coba running</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612115736.png" class="kg-image" alt="12-June-25" loading="lazy" width="932" height="310"></figure><h3 id="the-first-workflow-daily-quest">The first workflow | daily-quest</h3><p>lab : https://vscode.dev/tunnel/nb-ubuntu-desk/home/shezen/learn-gpt/first-workflow</p><p>Github action give tools to automation in repository git using <em>github_workflows</em>. It allow to runing task like build, test, deploy and push to production and minimize error/problem.</p><p><strong>Skenario</strong> : Create new repository, publish on github, add workflows to say &quot;Hello World&quot;</p><ol><li>Create new file <code>.github/workflows/first-workflows.yaml</code></li></ol><pre><code class="language-yaml file:first-workflows.yaml">name: Hello CI
on: 
    - push
jobs:
    build:
        runs-on: ubuntu-latest
        steps:
            - name: Checkout code
              uses: actions/checkout@v2
            - name: say hello
              run: echo &quot;Hello from CI/CD!&quot;
</code></pre><ol start="2"><li>Create repository <code>ci-cd-demo</code> on github</li></ol><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612231004.png" class="kg-image" alt="12-June-25" loading="lazy" width="880" height="716"></figure><ol start="3"><li>Git init, add, commit + push to remote repository</li></ol><pre><code class="language-sh">git init
git add .
git commit -m &quot;ci: add testing hello-world on gh_workflows&quot;

git remote add origin https://github.com/ngurah-bagus-trisna/ci-cd-demo.git
git branch -M main
git push -u origin main
</code></pre><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612231216.png" class="kg-image" alt="12-June-25" loading="lazy" width="951" height="989"></figure><p>4. Result</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020250612231216.png" class="kg-image" alt="12-June-25" loading="lazy" width="951" height="989"></figure><p>Answer :</p><ol><li>Blok on digunakan untuk penentu trigger sebuah action, jobs adalah sekumpulan action yang akan di running, steps adalah serangkaian langkah yang akan di exec dalam runner</li><li>Action checkout digunakan untuk mengambil data dalam repository</li></ol>]]></content:encoded></item><item><title><![CDATA[Install Kubecost]]></title><description><![CDATA[<p>Kubecost merupakan tools yang digunakan untuk tracing, monitoring biaya dari sebuah on-premises kubernetes cluster.</p><h2 id="installasi">Installasi</h2><p>Refrensi : https://www.kubecost.com/install#show-instructions</p><p>Installasi kubecost menggunakan helm, disini ketika melakukan installasi kubecost, secara otomatis akan terinstall prometheus dan grafana.</p><ol><li>Get helm values</li></ol><pre><code class="language-sh">helm repo add kubecost https://kubecost.github.io/cost-analyzer/</code></pre>]]></description><link>https://twnb.nbtrisna.my.id/install-kubecost/</link><guid isPermaLink="false">68560715dace4b00019e51f1</guid><category><![CDATA[catatan]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Mon, 09 Jun 2025 01:13:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/f93f74c8-4c16-4abe-b636-ee096b89a021.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://photoby.nbtrisna.my.id/f93f74c8-4c16-4abe-b636-ee096b89a021.jpg" alt="Install Kubecost"><p>Kubecost merupakan tools yang digunakan untuk tracing, monitoring biaya dari sebuah on-premises kubernetes cluster.</p><h2 id="installasi">Installasi</h2><p>Refrensi : https://www.kubecost.com/install#show-instructions</p><p>Installasi kubecost menggunakan helm, disini ketika melakukan installasi kubecost, secara otomatis akan terinstall prometheus dan grafana.</p><ol><li>Get helm values</li></ol><pre><code class="language-sh">helm repo add kubecost https://kubecost.github.io/cost-analyzer/
helm show values kubecost kubecost/cost-analyzer -n kubecost &gt; values.yaml
</code></pre><ol start="2"><li>Sesuaikan values</li></ol><p>Disini saya menyesuaikan mata uang yang digunakan dalam dashboard Kubecost</p><pre><code class="language-yaml">kubecostProductConfigs:
    currencyCode: IDR
</code></pre><p>Untuk service yang sebelumnya <code>ClusterIP</code>, saya set ke <code>NodePort</code> agar dapat mudah diakses.</p><pre><code class="language-yaml">service:
  type: NodePort 
  nodePort: 30003
</code></pre><ol start="3"><li>Installasi</li></ol><pre><code class="language-sh">helm upgrade --install kubecost \
  --repo https://kubecost.github.io/cost-analyzer/ cost-analyzer \
  --namespace kubecost --create-namespace -f values.yaml
</code></pre><p>Result, pastikan pod running semua.</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020241108195027.png" class="kg-image" alt="Install Kubecost" loading="lazy" width="1628" height="920"></figure><p>Url dapat diakses dengan nodeport :30003</p><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020241108195822.png" class="kg-image" alt="Install Kubecost" loading="lazy" width="3360" height="1854"></figure>]]></content:encoded></item><item><title><![CDATA[Docker Registry Mirror]]></title><description><![CDATA[<h3 id="refrensi">Refrensi :</h3><ul><li>https://stackoverflow.com/questions/74595635/how-to-configure-containerd-to-use-a-registry-mirror</li><li>https://docs.docker.com/registry/recipes/mirror/</li><li>https://docs.docker.com/registry/deploying/#deploy-your-registry-using-a-compose-file</li><li>https://d7y.io/docs/setup/runtime/containerd/mirror/</li></ul><p>Ketika sebuah ip terus&quot;an pull ke registry docker, akan menyebabkan sebuah peringatan limit pull telah terpakai. Biasanya akan reset</p>]]></description><link>https://twnb.nbtrisna.my.id/docker-registry-mirror/</link><guid isPermaLink="false">6856077bdace4b00019e51fd</guid><category><![CDATA[catatan]]></category><dc:creator><![CDATA[I Gusti Ngurah Bagus Trisna Andika]]></dc:creator><pubDate>Sun, 08 Jun 2025 01:14:00 GMT</pubDate><media:content url="https://photoby.nbtrisna.my.id/9239f368-b5a0-44d7-baa4-c06ae1572937.jpg" medium="image"/><content:encoded><![CDATA[<h3 id="refrensi">Refrensi :</h3><ul><li>https://stackoverflow.com/questions/74595635/how-to-configure-containerd-to-use-a-registry-mirror</li><li>https://docs.docker.com/registry/recipes/mirror/</li><li>https://docs.docker.com/registry/deploying/#deploy-your-registry-using-a-compose-file</li><li>https://d7y.io/docs/setup/runtime/containerd/mirror/</li></ul><img src="https://photoby.nbtrisna.my.id/9239f368-b5a0-44d7-baa4-c06ae1572937.jpg" alt="Docker Registry Mirror"><p>Ketika sebuah ip terus&quot;an pull ke registry docker, akan menyebabkan sebuah peringatan limit pull telah terpakai. Biasanya akan reset setelah 6 jam. Untuk menyiasati, dicoba riset install registry mirror di gns3.</p><h3 id="common-issue">Common issue</h3><ul><li>Tidak bisa login dengan auth htpasswd. syntax docker login bisa, hanya saja ketika pull tidak menggunakan registry-mirror</li><li>Hanya bisa 1 Registry per registry-mirror. Jadi ketika ingin cache image selain docker.io, harus membuat kembali docker-registry dengan port yang berbeda</li><li>Bisa untuk containerd, dan docker</li></ul><figure class="kg-card kg-image-card"><img src="https://img.nbtrisna.my.id/Pasted%20image%2020230529004247.png" class="kg-image" alt="Docker Registry Mirror" loading="lazy" width="857" height="607"></figure><p>Simple topology</p>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>hostname</th>
<th>IP</th>
</tr>
</thead>
<tbody>
<tr>
<td>regis-server</td>
<td>192.168.122.10/24</td>
</tr>
<tr>
<td>docker-client</td>
<td>192.168.122.11/24</td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="setup-registry-server">Setup registry server</h2><p>Metode yang dipakai adalah yang paling simple, menggunakan docker compose</p><p>Create self-signed certificate (For https)</p><pre><code class="language-sh">mkdir -p registry/{cert,auth,data}
cd registry

openssl req \
  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \
  -addext &quot;subjectAltName = DNS:registry.ngurah.local&quot; \
  -x509 -days 365 -out certs/ca.crt

sudo mkdir -p /etc/docker/certs.d/registry.ngurah.local:5000
sudo cp certs/ca.crt /etc/docker/certs.d/registry.ngurah.local:5000/
</code></pre><p>Or create letsencrypt certificate using dns challange.</p><pre><code class="language-sh">certbot certonly --preferred-challenges=dns --dns-cloudflare \
--server https://acme-v02.api.letsencrypt.org/directory \
--dns-cloudflare-credentials ~/.cloudflare.ini \
--agree-tos -d registry.ngurahbagus.my.id
</code></pre><p>Create User &amp; auth</p><pre><code class="language-sh">docker run \
  --entrypoint htpasswd \
  httpd:2 -Bbn ngurah wait &gt; auth/htpasswd
</code></pre><p>Create docker compose (if self-signed certificate)</p><pre><code class="language-yaml">version: &apos;2&apos;
services:
  registry:
    restart: always
    image: registry:2
    ports:
      - 5000:5000
    volumes:
      - /home/ubuntu/registry/data:/var/lib/registry
      - /home/ubuntu/registry/certs:/certs
      - /home/ubuntu/registry/auth:/auth
      - ./config.yml:/etc/docker/registry/config.yml
</code></pre><p>Create docker compose (if using lets-encrypt)</p><pre><code class="language-yaml">ersion: &apos;2&apos;
services:
  registry:
    restart: always
    image: registry:2
    ports:
      - 5000:5000
    volumes:
      - ./data:/var/lib/registry
      - /etc/letsencrypt/live/example.com/fullchain.pem:/etc/letsencrypt/live/example.com/fullchain.pem
      - /etc/letsencrypt/live/example.com/privkey.pem:/etc/letsencrypt/live/example.com/privkey.pem
      - ./auth:/auth
      - ./config.yml:/etc/docker/registry/config.yml
</code></pre><p>Create config.yml</p><pre><code class="language-yaml">version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
http:
  addr: :5000
  host: https://example.com
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: # your certificate
    key: # your cert key

health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
proxy:
  remoteurl: https://registry.k8s.io # mirroring k8s.io

</code></pre><p>Running docker container</p><pre><code class="language-sh">docker-compose up -d
</code></pre><p>Try login</p><pre><code class="language-sh">docker login registry.ngurah.local:5000
---
Login Succeeded # Berhasil
</code></pre><h2 id="setup-mirror-registry-client-docker">Setup mirror registry client Docker</h2><blockquote>Using Docker-client first</blockquote><p>Setup insecure registry</p><pre><code class="language-sh"># Skip if you not using self-signed cert
sudo mkdir -p /etc/docker/certs.d/registry.ngurah.local:5000
sudo cp ca.crt /etc/docker/certs.d/registry.ngurah.local:5000/
</code></pre><p>Configure mirror,</p><pre><code class="language-sh">sudo vi /etc/docker/daemon.json
---
{
  &quot;registry-mirrors&quot;: [&quot;https://registry.ngurah.local:5000&quot;]
}
</code></pre><p>Restart docker, &amp; try pull private image using credentials on registry-server</p><pre><code class="language-sh">sudo systemctl restart docker
docker pull hello-world 

# Check registry catalog
curl https://example.com:5000/v2/_catalog
---
{&quot;repositories&quot;:[&quot;kube-proxy&quot;,&quot;library/hello-world&quot;]} # makesure image availible
</code></pre><p>Sekarang image akan tercache di registry server</p><h2 id="setup-mirror-registry-client-containerd">Setup mirror registry client containerd</h2><p>Setup mirror first</p><pre><code class="language-sh">mkdir /etc/containerd/certs.d/registry.k8s.io
sudo vi /etc/containerd/certs.d/registry.k8s.io/hosts.toml
---
server = &quot;https://registry.k8s.io&quot;

[host.&quot;https://example:5000&quot;]
  capabilities = [&quot;pull&quot;, &quot;resolve&quot;]
</code></pre><p>Konfigurasi /etc/containerd/config.toml untuk membaca registry di certs.d</p><pre><code class="language-yaml">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry] 
      config_path = &quot;/etc/containerd/certs.d&quot; # enable
</code></pre><p>Restart containerd &amp; try pull registry.k8s.io</p><pre><code class="language-sh">sudo systemctl restart containerd.io
sudo crictl pull registry.k8s.io/kube-proxy:v1.27.2
</code></pre>]]></content:encoded></item></channel></rss>